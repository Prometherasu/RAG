[
  {
    "chunk_id": 0,
    "content": "Pre p rint. Under review.\n## **HyperRAG: Enhancing Quality-Efficiency Tradeoffs in** **Retrieval-Augmented Generation with Reranker KV-Cache** **Reuse**\n\n**Yuwei An** [1], **Yihua Cheng** [2], **Seo Jin Park** [3], **Junchen Jiang** [2]",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 1,
    "content": "1 Carnegie Mellon University\n2 University of Chicago\n3 University of Southern California\n### **Abstract**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 2,
    "content": "Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm for enhancing the performance of large language models (LLMs)\nby integrating external knowledge into the generation process. A key component of RAG pipelines is the reranker, which selects the most relevant",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 3,
    "content": "documents from a pool of retrieved candidates and significantly improves\nthe quality of the generated responses. While rerankers refine the selection\nof retrieved documents in RAG pipelines, they introduce computational\nchallenges that hinder high throughput and low latency. To address this",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 4,
    "content": "problem, we propose HyperRAG, a system that optimizes the trade-off between quality and efficiency in RAG pipelines by leveraging KV-cache reuse\nfor efficient reranker inference. By reusing document-side KV-cache, HyperRAG achieves both high-quality generation and system-level efficiency.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 5,
    "content": "To fully realize the benefits of KV-cache reuse, HyperRAG incorporates a\nrange of system-level optimizations designed to enhance efficiency and\nscalability. Experiments show that HyperRAG achieves a 2–3× throughput\nimprovement with decoder-only rerankers while also delivering higher",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 6,
    "content": "downstream performance compared with traditional RAG service.\n### **1 Introduction**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 7,
    "content": "Retrieval-Augmented Generation (RAG) (Gao et al., 2024) has emerged as a powerful\nparadigm that enhances the performance of large language models (LLMs) by incorporating\nexternal knowledge into the generation process. RAG systems typically follow a two-stage",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 8,
    "content": "pipeline: retrieval and generation. Among these, the retrieval stage plays a critical role as\nthe relevance and quality of the retrieved documents significantly influence the final output.\nSelecting the most appropriate documents from a large corpus determines how effectively",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 9,
    "content": "the model can respond to queries, especially in open-domain settings.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 10,
    "content": "Compared with the coarse-grained retrieval which is based on dense embedding similarity\nover the vector index (Izacard et al., 2022; Chen et al., 2024), the reranker performs finegrained selection by scoring candidates with richer contextual understanding, leading",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 11,
    "content": "to more relevant and concise results (Reimers & Gurevych, 2019; Gao & Callan, 2022).\nHowever, the reranking step introduces additional computational overhead and latency\nwhich can become a bottleneck in high-throughput RAG systems. This issue is further",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 12,
    "content": "exacerbated by the adoption of large language model (LLM)-based rerankers (Chen et al.,\n2024; Ma et al., 2023; Pradeep et al., 2023). On one hand, these rerankers are fine-tuned from\npowerful pre-trained generative models and achieve state-of-the-art performance on various",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 13,
    "content": "retrieval tasks, such as passage ranking and document selection. On the other hand, they\nare computationally intensive and introduce substantial latency, especially when handling\nlarge batches of queries. As a result, despite their strong performance, LLM-based rerankers",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 14,
    "content": "are often impractical for real-time RAG applications.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 15,
    "content": "1\n\n\n-----\n\nPre p rint. Under review.\n\nSo the core challenge we aim to address is:",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 16,
    "content": "So the core challenge we aim to address is:\n\n*How can we optimize the trade-off between generation quality and efficiency in RAG systems,*\n*especially with large-scale LLM-based rerankers, to deliver effective results without significantly*\n*compromising system throughput?*",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 17,
    "content": "To address this challenge, we propose HyperRAG, a system design that leverages efficient\nKV-cache management to optimize the quality-efficiency trade-off in the RAG system.\nSpecifically, HyperRAG introduces a KV-cache storage mechanism that stores the KV-cache",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 18,
    "content": "of all document chunks. When performing query and document reranking, HyperRAG\nefficiently loads the precomputed KV-cache, eliminating the need for recomputation and\nreducing latency while maintaining high generation quality. This mechanism shifts the",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 19,
    "content": "bottleneck from GPU computation to SSD storage and transfer bandwidth, effectively\nbalancing the workload and enabling more efficient utilization of system resources.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 20,
    "content": "The main contribution of HyperRAG includes:",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 21,
    "content": "- **Highlighting the potential of decoder based reranking** : HyperRAG emphasizes\nthe critical importance of rerankers in improving generation quality within RAG\npipelines. Our empirical findings show that incorporating powerful decoder-based",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 22,
    "content": "rerankers significantly enhances downstream performance, revealing the untapped\npotential of reranking in practical RAG deployments.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 23,
    "content": "- **Enabling efficient decoder-based reranking via KV-cache reuse** : To address the\ncomputational inefficiency problem associated with decoder-based rerankers, HyperRAG introduces a KV-cache reuse mechanism. By caching the document-side",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 24,
    "content": "key/value pairs, the reranker only needs to process the query portion during inference. This shifts the bottleneck from GPU compute to underutilized resources such\nas NVM storage and PCIe bandwidth, enabling high-throughput inference while\nmaintaining strong generation quality.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 25,
    "content": "Our experiments demonstrate that HyperRAG maintains high-quality generation while\ndelivering a 2–3× throughput improvement during RAG service. This design paves the way\nfor a new pattern for deploying RAG systems.\n### **2 Background**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 26,
    "content": "In this section, we introduce the background and related work underlying the design of\nHyperRAG.\n\n**2.1** **Retrieval-Augmented Generation**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 27,
    "content": "Retrieval-Augmented Generation (RAG) (Ram et al., 2023; Lewis et al., 2021; Asai et al.,\n2023; Khandelwal et al., 2020; Jin et al., 2024a; Shao et al., 2024) is the process of optimizing\nthe output of a large language model and it references an authoritative knowledge base",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 28,
    "content": "outside of its training data sources before generating a response. This paradigm has gained\ntraction for tasks requiring factual accuracy and up-to-date information, such as question\nanswering, summarization, and dialogue generation.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 29,
    "content": "Traditional RAG pipelines rely on retrievers\nwhich depend on embedding representations and cosine similarity to fetch relevant\ndocuments (Robertson & Zaragoza, 2009;\nReimers & Gurevych, 2019; Wang et al.,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 30,
    "content": "Reimers & Gurevych, 2019; Wang et al.,\n2024). While this approach is straightforward, it often struggles to achieve optimal results in more complex scenarios. To",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 31,
    "content": "Figure 1: Classic RAG Workflow: The query\n\nsolve the problem, advancements have in\nis embedded and used to retrieve top-K doc\ntroduced reranker mechanisms that refine\n\numents. Then the reranker selects the most\n\nthe retrieved documents to improve rele\nrelevant ones which are combined with the",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 32,
    "content": "vance and contextuality before generation.\n\nquery to generate the final response.\n\n2\n\n\n-----\n\nPre p rint. Under review.\n\n(a) TriviaQA (b) NaturalQA (c) PopQA",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 33,
    "content": "Figure 2: RAG downstream performance with different rerankers: Subfigures a, b, and c\nshow the performance curves of exact match (EM) scores on TriviaQA (Joshi et al., 2017), NaturalQA (Kwiatkowski et al., 2019), and PopQA (Mallen et al., 2023) with various rerankers.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 34,
    "content": "The x-axis denotes the number of retrieved documents involved during reranking from\nwhich the top-1 document is selected for generation. **D-BOUND** represents the performance\nupper bound is based on the number of documents during rerank while **M-BOUND** reflects",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 35,
    "content": "that the upper bound is determined by the reranker’s ability to identify the most relevant\ndocument. The generation model is meta-llama/Llama-3.1-8B-Instruct . The five labels\nrepresent different configurations: Baseline (No RAG), Embedding-only (retrieves the top",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 36,
    "content": "document directly using cosine similarity), E/MINILM (uses the ms-marco-MiniLM-L6v2 reranker which is Encoder-only (Face, 2025)), E/BGEM (uses the bge-reranker-v2-m3\nreranker which is Encoder-only (of Artificial Intelligence, BAAI)), and D/GEMMA (uses",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 37,
    "content": "the Gemma 2B reranker which is Decoder-only (of Artificial Intelligence, BAAI)).",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 38,
    "content": "These rerankers, often transformer-based,\nsignificantly boost the quality of generated content by ensuring the most pertinent documents are prioritized.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 39,
    "content": "Early Rerankers were predominantly trained on encoder-only models such as BERT (Devlin\net al., 2019) or XLM-RoBERTa (Conneau et al., 2020), leveraging their strong encoding capabilities to improve retrieval precision. However, recent advancements have demonstrated",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 40,
    "content": "the growing dominance of decoder-based rerankers which capitalize on the powerful generative language capabilities of modern decoder models (Chen et al., 2024; Ma et al., 2023;",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 41,
    "content": "Pradeep et al., 2023). By fine-tuning these decoder-based models on tasks originally designed for encoder-only rerankers, they achieve significant gains in performance, benefiting\nfrom both their inherent generative power and the fine-grained contextual understanding\nacquired during fine-tuning.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 42,
    "content": "**2.2** **KV-cache Reuse of LLM**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 43,
    "content": "In generation scenario, KV-cache reuse has emerged as a vital optimization technique for\nimproving the efficiency of large language models (LLMs) during inference. (Liu et al.,\n2024). In RAG scenarios, prior works mostly focus on reusing the KV-cache of the retrieved",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 44,
    "content": "documents to speed up the generation stage (Yao et al., 2024; Gim et al., 2024; Jin et al.,\n2024b). In this work, we focus on reusing KV-cache during the retrieval stage to speed up\nthe rerank process.\n### **3 Observation**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 45,
    "content": "In this section, we highlight the key observations that inspired the design of HyperRAG.\nIn Section 3.2, we present the quality-efficiency trade-off curve inherent in RAG serving\nsystems which our design aims to address. This trade-off is primarily influenced by the",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 46,
    "content": "computational cost of embedding retrieval and reranking. In Section 3.3, we explore the\nbenefits of KV cache reuse during reranking inference and explain why reranking is the\noptimal scenario for leveraging KV cache reuse.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 47,
    "content": "3\n\n\n-----\n\nPre p rint. Under review.\n\n(a) Latency vs EM (b) Full vs Reuse (c) Memory Footprint (d) Throughput vs batch",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 48,
    "content": "Figure 3: Efficiency Observations for the Reranker: Subfigure 3(a) illustrates the trade-off between latency and NaturalQA performance across different reranker models. Subfigure 3(b)\npresents the latency and throughput of the Gemma-2B reranker under varying document",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 49,
    "content": "chunk sizes, with the query chunk size fixed at 48. The blue line indicates full computation,\nwhile the orange line represents computation with KV-cache reuse. Solid lines denote\nlatency, and dashed lines denote throughput. Subfigure 3(c) shows the memory footprint",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 50,
    "content": "of the Gemma-2B reranker during inference with different batch sizes, using a fixed input\nlength of 256 + 48 = 304. Subfigure 3(d) highlights how throughput increases with larger\nbatch sizes up until an out-of-memory (OOM) error occurs for Gemma-2B reranker model\ninference.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 51,
    "content": "**3.1** **Reranker Empowers RAG**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 52,
    "content": "In retrieval-augmented generation (RAG), the quality of the retrieved documents directly\ninfluences the final generation output. However, the initial retrieval step often returns a set\nof candidate documents with mixed relevance. A reranker plays a critical role in reordering",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 53,
    "content": "these candidates to identify the most relevant ones, enabling the model to generate more\naccurate and grounded responses.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 54,
    "content": "Our findings show that a powerful reranker is essential to fully realize the potential of\nRAG. Without it, even high-capacity language models struggle to make effective use of\nthe retrieved context. As shown in Figure 2, increasing the number of candidates reranked",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 55,
    "content": "improves the performance ceiling (D-BOUND), but only a strong reranker can consistently\nselect the best document and push the system closer to the model’s generation upper bound\n(M-BOUND). This observation highlights the reranker’s pivotal role in bridging retrieval",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 56,
    "content": "and generation and demonstrates that a RAG system can fully realize its potential only\nwhen equipped with a powerful reranker.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 57,
    "content": "**3.2** **Quality-Efficiency Trade-off**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 58,
    "content": "While larger rerankers deliver better generation quality, they are typically built on large\nlanguage models and incur significantly higher latency. As shown in Figure 3(a), more powerful rerankers such as Gemma-2B reranker model achieve higher EM scores on NaturalQA",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 59,
    "content": "but at a much greater computational cost compared to lighter-weight encoder models.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 60,
    "content": "This trade-off becomes particularly problematic in real-world deployment scenarios. For\neach incoming query, the reranker must evaluate all top- *k* retrieved documents by pairing the\nquery with each candidate and processing them through the model. To improve throughput,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 61,
    "content": "multiple queries are typically batched together within the RAG service pipeline, resulting in\nexceptionally large batch sizes. Under these conditions, rerankers based on large language\nmodels suffer from severe performance degradation compared to their smaller counterparts.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 62,
    "content": "This efficiency bottleneck is a key reason why many production RAG systems avoid using\nthese more powerful rerankers despite their superior retrieval quality.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 63,
    "content": "**3.3** **KV-Cache Reuse**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 64,
    "content": "Based on the above observation, the key challenge is to leverage powerful reranker models\nwhile mitigating the service efficiency degradation they introduce. Given the substantial\ncomputational overhead of large language model inference, reusing the KV-cache has",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 65,
    "content": "emerged as a promising strategy for accelerating decoder-based models. While prior work",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 66,
    "content": "4\n\n\n-----\n\nPre p rint. Under review.\n\nhas primarily focused on applying KV-cache reuse during the generation phase, we argue\nthat extending this technique to the reranking stage is both more efficient and better aligned\nwith its characteristics.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 67,
    "content": "In this section, we outline several important **properties** of reranking that make it suitable\nfor KV-cache reuse, and highlight the **potential** benefits this approach can bring in terms of\nefficiency improvements.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 68,
    "content": "**Property 1: Lossless** Compared to encoder-based rerankers, decoder-based rerankers benefit\nfrom the tri-mask mechanism, which enables lossless two-stage inference. Specifically, this\nmechanism ensures that computing the score for a ”document + query” pair yields the same",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 69,
    "content": "result as first performing inference on the document alone and then processing the query\nusing the precomputed KV-cache from the document.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 70,
    "content": "Another key advantage is that reranker inference operates on a document-query pair basis,\nevaluating one document chunk against a query at a time. This pairwise structure eliminates\na major challenge present in KV-cache reuse during the generation phase of RAG where",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 71,
    "content": "different orderings of documents (e.g., *Document A + Document B + Query* vs. *Document B +*\n*Document A + Query* ) result in different KV-cache and necessitate costly recomputation. In\ncontrast, reranking avoids this issue entirely since each document is scored independently",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 72,
    "content": "with respect to the query.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 73,
    "content": "**Property 2: Static** The chunk size of the document is often fixed, which brings significant\nbenefits for KV-cache management and inference compilation. By maintaining a consistent\nchunk size, the KV-cache structure becomes predictable which simplifies memory allocation",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 74,
    "content": "and reduce unnecessary padding.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 75,
    "content": "**Property 3: Large Reuse Ratio** In the reranking scenario, the token length of document\nchunks is typically larger than that of the query. This results in a high reuse ratio, which\nsignificantly reduces latency since the majority of tokens can be reused without recalculation.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 76,
    "content": "These favorable properties make reranking an especially suitable scenario for applying\nKV-cache reuse. Our observations further reveal the untapped potential of this approach to\nsignificantly improve efficiency without compromising accuracy.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 77,
    "content": "**Potential 1: Less Computation** Thanks to the high reuse ratio enabled by KV-cache reuse,\nreranking with decoder-based models can significantly reduce redundant computation.\nInstead of recomputing document representations for each query, the cached document-side",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 78,
    "content": "KV states can be reused across different document–query pairs. Figure 3(b) demonstrates\nthis effect using the Gemma-2B reranker. As chunk size increases, the latency of full inference\ngrows rapidly due to the increasing computation required for processing longer sequences.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 79,
    "content": "In contrast, with KV-cache reuse, latency remains nearly constant and substantially lower\nacross all chunk sizes. This indicates that the document-side computation—normally the\ndominant contributor to latency—has been effectively amortized. Naturally, the throughput",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 80,
    "content": "of reranking also improves as a result of the reduced latency.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 81,
    "content": "**Potential 2: Lower Memory Footprint Enables Larger Batches** KV-cache reuse also offers\nsubstantial memory savings, which can translate into significant throughput improvements.\nFigure 3(c) shows the memory footprint of the prefill stage when running the Gemma-2B",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 82,
    "content": "reranker. As the chunk size increases, the intermediate activations during full prefilling\nbecome large and quickly exhaust available GPU memory. This limits the maximum batch\nsize that can be processed concurrently. Figure 3(d) demonstrates that batch size can be a key",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 83,
    "content": "bottleneck for throughput—when memory is constrained, which means that the memory\nupper bound comes first compared to compuation upper bound.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 84,
    "content": "In contrast, KV-cache reuse significantly reduces the intermediate memory requirements\nby skipping document-side prefilling during inference. With reduced memory usage, the\nreranker can support higher levels of parallelism, enabling larger batch sizes and increased\nthroughput during reranking.\n\n5",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 85,
    "content": "5\n\n\n-----\n\nPre p rint. Under review.\n### **4 HyperRAG**\n\n\nFigure 4: Overview of HyperRAG",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 86,
    "content": "In this section, we present the overall system design of **HyperRAG**, our high-performance\nretrieval-augmented generation (RAG) framework optimized for both efficiency and scalability. The rest of this section is organized as follows. Section 4.1 introduces the design and",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 87,
    "content": "benefits of KV-cache compression in HyperRAG. Sections 4.2, 4.3, and 4.4 then discuss the\nhierarchical design of HyperRAG, moving from GPU to CPU and finally to Storage.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 88,
    "content": "**4.1** **KV-cache Compression: Addressing the Storage Bottleneck**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 89,
    "content": "While KV-cache reuse offers significant latency and throughput benefits, it introduces a new\nset of challenges, particularly when scaling to large datastores. In this scenario, the system\nmay need to offload KV-cache to external storage devices such as NVMe or cloud-based",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 90,
    "content": "object stores. As a result, the bottleneck shifts from GPU computation to the storage and data\ntransfer layers. Two major issues emerge: the overall size of the KV-cache and the bandwidth\nrequired to transfer it between storage and GPU memory. To address this, we propose",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 91,
    "content": "lightweight KV-cache compression techniques that are critical for scaling HyperRAG to\nlarge deployments.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 92,
    "content": "**Efficient Attention Mechanisms:** Compared to traditional attention mechanisms, recent\ndesigns such as Grouped Query Attention (GQA) (Ainslie et al., 2023) and Multi-Latent",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 93,
    "content": "Attention (MLA) (DeepSeek-AI et al., 2024) significantly reduce KV-cache memory requirements per token. While originally developed to improve GPU decoding throughput, these\nmechanisms are particularly beneficial in the context of HyperRAG. Since reranker inference",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 94,
    "content": "involves transferring KV-cache for every document–query pair, reducing the per-token KV\nfootprint directly eases pressure on storage bandwidth and memory usage. Among the\nrerankers we evaluated, the Gemma-2B reranker is especially well-suited for this setting",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 95,
    "content": "due to its low KV memory cost per token, making it ideal for large-scale and cache-intensive\nRAG inference.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 96,
    "content": "**KV-cache Quantization:** Quantization (Han et al., 2016; Lin et al., 2024; Xiao et al., 2024)\nis a widely-used technique for compressing model weights and activations by reducing\nprecision. In HyperRAG, we extend this concept to KV-cache, focusing on minimizing",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 97,
    "content": "storage and transfer cost while preserving model performance. Rather than applying full\n6",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 98,
    "content": "-----\n\nPre p rint. Under review.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 99,
    "content": "model quantization schemes like **W8A8** or **W4A16**, we adopt specialized formats such as\n**KV8W16A16** and **KV4W16A16**, where quantization is applied exclusively to KV-cache\nwhile keeping model weights and activations in higher precision. However in the following",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 100,
    "content": "experiment we do not use quantization further. Appendix B shows the reason why we do\nnot use it for Gemma-2B reranker.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 101,
    "content": "**4.2** **GPU-Centric Computation**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 102,
    "content": "In HyperRAG, GPU resources are responsible for two main computational roles: (1) retrievalside processing—including embedding generation and reranking—and (2) generation-side\ninference, where the top-ranked document is combined with the query to produce the final\nresponse.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 103,
    "content": "To maximize system throughput, it is crucial to balance the computational load between these two stages. If either the reranking or generation side becomes a bottleneck,\noverall throughput suffers due to underutilized GPU resources. HyperRAG addresses",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 104,
    "content": "this by disaggregating retrieval and gen- Figure 5: Static KV Layout: During reranking,\neration pipelines and carefully managing we allocate a fixed-length KV buffer for attenGPU allocation. By dynamically tuning the tion. The buffer consists of a static document",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 105,
    "content": "number of GPUs assigned to each stage and segment (retrieved KV, shown in red) and a\nleveraging KV-cache reuse to reduce redun- static query segment (computed KV, shown in\ndant computation, HyperRAG maintains a green).\nbalanced and efficient end-to-end inference\npipeline.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 106,
    "content": "In addition to load balancing, HyperRAG further optimizes GPU execution by adopting\na **static attention layout**, enabling better fusion and graph capture through torch compile\nand CUDA graphs (Ansel et al., 2024). As illustrated in Figure 5, we fix the structure of the",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 107,
    "content": "attention kv-input such that the document occupies a static prefix region while the query\noccupies a fixed-length suffix. The document tokens use precomputed KV-cache (retrieved\nfrom memory) and only the query portion contributes to new KV computation. This static",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 108,
    "content": "partitioning allows the entire decoding process to be compiled ahead of time and executed\nwith minimal runtime overhead, greatly improving inference efficiency.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 109,
    "content": "**4.3** **CPU-Centric Index**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 110,
    "content": "In the Embedding retrieval stage of HyperRAG, we leverage FAISS, a highly optimized\nlibrary for efficient similarity search on dense vectors. This CPU-centric approach ensures",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 111,
    "content": "the rapid indexing and retrieval of top-K documents. FAISS operates on the dense embeddings generated by the model, implementing approximate nearest neighbor (ANN) search",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 112,
    "content": "algorithms that are both scalable and precise. Specifically, we employ IVF (Inverted File Index) and HNSW (Hierarchical Navigable Small World) (Malkov & Yashunin, 2018) indexing\nstructures within FAISS (Douze et al., 2025) which enables fast and memory-efficient search",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 113,
    "content": "while maintaining high recall.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 114,
    "content": "**4.4** **Storage Backend**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 115,
    "content": "To support efficient and scalable KV-cache access in HyperRAG, we build our storage\nbackend on top of the open-source *LMCache* framework (Cheng et al., 2024). This backend\nis designed to provide near-constant KV-cache retrieval latency, even as the total cache size",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 116,
    "content": "scales to billions of tokens. It offers fast and reliable KV-cache management across multiple\nstorage backends, such as local NVMe drives and remote systems like Redis.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 117,
    "content": "A key feature of our storage layer is its ability to seamlessly partition the KV-cache across\nmultiple storage devices, such as independent NVMe drives or remote object storage.\nGiven the scale of KV-cache required for large datastores, such partitioning is essential.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 118,
    "content": "To maintain retrieval efficiency in this setting, we align the partitioning strategy with the",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 119,
    "content": "7\n\n\n-----\n\nPre p rint. Under review.\n\n**BGE-M3** **Gemma** **H-Gemma**\nC Metrics\n\n|Col1|Col2|5 10 20|5 10 20|5 10 20|\n|---|---|---|---|---|",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 120,
    "content": "|P|TriviaQA NQ PopQA Throughput|55.80 57.41 58.30 25.31 26.08 26.83 30.89 31.53 32.11 165.3 124.7 67.8|56.95 59.55 59.66 27.08 29.00 30.03 32.19 34.13 36.45 51.2 27.4 13.3|56.95 59.55 59.66 27.08 29.00 30.03 32.19 34.13 36.45 78.8 50.9 28.9|\n|---|---|---|---|---|",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 121,
    "content": "|D|TriviaQA NQ PopQA Throughput|56.15 56.92 57.33 25.13 25.87 26.60 29.98 30.62 31.21 75.8 69.0 41.7|56.91 58.77 59.05 26.55 28.47 29.50 29.98 32.02 34.21 28.4 15.2 7.5|56.91 58.77 59.05 26.55 28.47 29.50 29.98 32.02 34.21 48.7 36.2 21.1|\n|---|---|---|---|---|",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 122,
    "content": "Table 1: Downstream Performance Evaluation: We report Exact Match (EM) scores on\nthree QA benchmarks along with throughput across different settings. The column labeled\nC denotes the corpus type: Passage-level (P) or Document-level (D). For each reranker",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 123,
    "content": "model (BGE-M3 (of Artificial Intelligence, BAAI), Gemma (of Artificial Intelligence, BAAI),\nHyperRAG Gemma(H-Gemma)), we vary the number of retrieved documents involved in\nreranking (shown as 5, 10, and 20).",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 124,
    "content": "indexing structure of the embedding retriever. Specifically, we group KV-cache entries by\nthe *coarse centroid ID* used in the FAISS IVF index. This means that all KV entries assigned\nto the same coarse cluster are stored within the same storage backend.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 125,
    "content": "This design offers a critical performance benefit: since approximate nearest neighbor (ANN)\nsearch during retrieval only probes a small number of centroid clusters, the corresponding\nKV-cache retrieval is limited to a single backend. As a result, we avoid cross-device fetching,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 126,
    "content": "reduce latency, and improve I/O locality.\n### **5 Experiment**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 127,
    "content": "In this section, we present the results of the benchmark and performance experiments.\n\n**5.1** **Experiment settings**\n\n**Corpus** :We evaluate HyperRAG under two different levels of retrieval granularity: passagelevel and document-level.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 128,
    "content": "*Passage-Level Corpus:* For the passage-level setting, we use the psgs multiset-100 dataset as\nour base corpus. To ensure consistent input lengths and improve KV cache reuse efficiency,\nwe rechunk all passages to a fixed size of approximately 200 words per chunk. In this case",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 129,
    "content": "document tokens length is fixed as 256.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 130,
    "content": "*Document-Level Corpus:* For the document-level setting, we adopt the MS MARCO (Bajaj\net al., 2018) document dataset. Each document is chunked into fixed-length segments with a\nmaximum word length of 450. In this case document tokens length is fixed as 512.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 131,
    "content": "**Metrics** : To evaluate downstream performance, we utilized TriviaQA (Joshi et al., 2017),\nNaturalQA (Kwiatkowski et al., 2019), and PopQA (Mallen et al., 2023) as benchmarks\nfor performance evaluation. For the benchmark measurement, we collect the average",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 132,
    "content": "throughput of RAG service(request per second) after warm-up for efficiency evaluation.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 133,
    "content": "**Model** : For the embedding model of index search, we used the contriever (Izacard et al.,\n2022) model. In the generation stage, we employed the Llama3.1-8B-Instruct model\n(Grattafiori et al., 2024).",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 134,
    "content": "**Hardware** The experiments were conducted on a system equipped with 8 NVIDIA A100\nGPUs. For storage backend we use redis remote backend(4TB) and local nvme backend(4TB).\nWe store all the hot KV-cache that will be used during inference on the QA dataset and then",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 135,
    "content": "fulfill the whole storage with random sampling.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 136,
    "content": "8\n\n\n-----\n\nPre p rint. Under review.\n\n**5.2** **Trade-off Performance**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 137,
    "content": "We conduct experiments to evaluate both downstream performance and system efficiency.\nThe results, presented in Table 1, demonstrate that HyperRAG maintains high-quality\ngeneration while achieving a 2–3× improvement in service throughput on average. These",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 138,
    "content": "findings highlight HyperRAG’s ability to effectively balance generation quality and costefficiency, making it a robust and practical solution for RAG service.\n### **6 Discussion**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 139,
    "content": "In this section we will provide some key discussions around HyperRAG.\n\n**6.1** **RAG Service Framework**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 140,
    "content": "One of the recent hot topics in RAG research is enabling the generation model to better\nidentify and utilize the most relevant retrieved documents (Asai et al., 2023; Wei et al.,\n2025). HyperRAG aligns with this research direction, but with a key distinction: instead of",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 141,
    "content": "relying solely on the generation model to perform this filtering, we delegate the relevance\ndiscrimination task to a powerful reranker. This reranker operates with similar mechanisms\nto large language models but is significantly more efficient due to its smaller size and the",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 142,
    "content": "support of HyperRAG’s architecture. Moreover, this separation of relevance extraction\noffers greater flexibility during deployment.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 143,
    "content": "**6.2** **Real-World Application**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 144,
    "content": "Storing reranker KV caches introduces a significant storage requirement. For example, we\nestimate that storing the full KV cache for the Gemma-2B reranker over the MS MARCO\ndocument dataset would require more than 40TB of storage. This raises questions about",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 145,
    "content": "the practicality of deploying HyperRAG in real-world settings. However, we argue that\nHyperRAG remains a viable and efficient solution for two main reasons.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 146,
    "content": "**Production Stack Design:** Figure 6 illustrates a potential distributed deployment\narchitecture for HyperRAG. In this setup,\nthe KV cache backend functions as a shared\nmemory store accessible by multiple RAG\nservice engines. This design allows the storage burden to be amortized across multiple",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 147,
    "content": "applications and services, enabling efficient\nand centralized knowledge sharing at scale.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 148,
    "content": "**Financial Cost:** In practice, storage costs are\nnegligible compared to GPU infrastructure. Figure 6: Production Stack: The stored KV\nFor instance, on Amazon Web Services, the cache backend can be shared across multiple\ncost of provisioning 8×A100 GPUs far ex- RAG service engines.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 149,
    "content": "ceeds that of acquiring 40TB of persistent\nstorage. By introducing a relatively small additional cost for storage, HyperRAG achieves\nsignificantly higher throughput—making it a more cost-effective solution from the perspective of service providers aiming to optimize performance per dollar.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 150,
    "content": "### **References**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 151,
    "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, ´\nand Sumit Sanghai. Gqa: Training generalized multi-query transformer models from\n[multi-head checkpoints, 2023. URL https://arxiv.org/abs/2305.13245.](https://arxiv.org/abs/2305.13245)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 152,
    "content": "Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali\nChourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng,\n\n9\n\n\n-----\n\nPre p rint. Under review.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 153,
    "content": "Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar,\nLaurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu,\nC. K. Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 154,
    "content": "Marcos Yukio Siraichi, Helen Suk, Shunting Zhang, Michael Suo, Phil Tillet, Xu Zhao,\nEikan Wang, Keren Zhou, Richard Zou, Xiaodong Wang, Ajit Mathews, William Wen,\nGregory Chanan, Peng Wu, and Soumith Chintala. Pytorch 2: Faster machine learning",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 155,
    "content": "through dynamic python bytecode transformation and graph compilation. In *Proceedings*\n*of the 29th ACM International Conference on Architectural Support for Programming Languages*",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 156,
    "content": "*and Operating Systems, Volume 2*, ASPLOS ’24, pp. 929–947, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703850. doi: 10.1145/3620665.3640366.\n[URL https://doi.org/10.1145/3620665.3640366.](https://doi.org/10.1145/3620665.3640366)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 157,
    "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag:\nLearning to retrieve, generate, and critique through self-reflection, 2023. URL [https:](https://arxiv.org/abs/2310.11511)\n[//arxiv.org/abs/2310.11511.](https://arxiv.org/abs/2310.11511)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 158,
    "content": "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song,\nAlina Stoica, Saurabh Tiwary, and Tong Wang. Ms marco: A human generated machine",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 159,
    "content": "[reading comprehension dataset, 2018. URL https://arxiv.org/abs/1611.09268.](https://arxiv.org/abs/1611.09268)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 160,
    "content": "Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge\nm3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings\n[through self-knowledge distillation, 2024. URL https://arxiv.org/abs/2402.03216.](https://arxiv.org/abs/2402.03216)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 161,
    "content": "Yihua Cheng, Kuntai Du, Jiayi Yao, and Junchen Jiang. Do large language models need a\ncontent delivery network? *arXiv preprint arXiv:2409.13761*, 2024.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 162,
    "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume\nWenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin ´\nStoyanov. Unsupervised cross-lingual representation learning at scale, 2020. URL [https:](https://arxiv.org/abs/1911.02116)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 163,
    "content": "[//arxiv.org/abs/1911.02116.](https://arxiv.org/abs/1911.02116)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 164,
    "content": "DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao,\nChengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji,\nErhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 165,
    "content": "Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui\nLi, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang\nYuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 166,
    "content": "Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan\nZhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi\nWang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 167,
    "content": "Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang\nChen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping\nYu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 168,
    "content": "Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q.\nLi, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen,\nXiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 169,
    "content": "Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su,\nY. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao,\nYaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 170,
    "content": "Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang,\nYongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma,\nYuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 171,
    "content": "Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu,\nZhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseekv2: A strong, economical, and efficient mixture-of-experts language model, 2024. URL",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 172,
    "content": "[https://arxiv.org/abs/2405.04434.](https://arxiv.org/abs/2405.04434)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 173,
    "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding, 2019. URL [https://arxiv.](https://arxiv.org/abs/1810.04805)\n[org/abs/1810.04805.](https://arxiv.org/abs/1810.04805)\n\n10\n\n\n-----",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 174,
    "content": "10\n\n\n-----\n\nPre p rint. Under review.\n\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herv ´ e J ´ egou. The faiss library, ´\n[2025. URL https://arxiv.org/abs/2401.08281.](https://arxiv.org/abs/2401.08281)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 175,
    "content": "Hugging Face. cross-encoder/ms-marco-minilm-l6-v2. [https://huggingface.co/](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2)\n[cross-encoder/ms-marco-MiniLM-L6-v2, 2025. Accessed: March 29, 2025.](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 176,
    "content": "Luyu Gao and Jamie Callan. Long document re-ranking with modular re-ranker. In\n*Proceedings of the 45th International ACM SIGIR Conference on Research and Development in*\n*Information Retrieval*, SIGIR ’22, pp. 2371–2376. ACM, July 2022. doi: 10.1145/3477495.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 177,
    "content": "[3531860. URL http://dx.doi.org/10.1145/3477495.3531860.](http://dx.doi.org/10.1145/3477495.3531860)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 178,
    "content": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\nMeng Wang, and Haofen Wang. Retrieval-augmented generation for large language\n[models: A survey, 2024. URL https://arxiv.org/abs/2312.10997.](https://arxiv.org/abs/2312.10997)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 179,
    "content": "In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong.\nPrompt cache: Modular attention reuse for low-latency inference. *Proceedings of Machine*\n*Learning and Systems*, 6:325–338, 2024.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 180,
    "content": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy\nYang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 181,
    "content": "Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 182,
    "content": "Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer,\nCyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny\nWyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 183,
    "content": "Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan,\nEric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, ´\nGabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 184,
    "content": "Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron,\nIliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack\nZhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 185,
    "content": "Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak,\nJongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 186,
    "content": "Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin\nStone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal\nLakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 187,
    "content": "Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke\nde Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin\nKardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 188,
    "content": "Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal,\nNarjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang,\nOlivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 189,
    "content": "Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,\nPuxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 190,
    "content": "Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 191,
    "content": "Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 192,
    "content": "Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 193,
    "content": "Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish\nVogeti, V ´ ıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney\nMeers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 194,
    "content": "11\n\n\n-----\n\nPre p rint. Under review.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 195,
    "content": "Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei,\nYi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert,\nZheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 196,
    "content": "Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand,\nAjay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda\nKallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 197,
    "content": "Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani,\nAnnie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley\nGabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 198,
    "content": "Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing\nLiu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 199,
    "content": "Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris\nTindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 200,
    "content": "Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana\nLiskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa\nJamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 201,
    "content": "Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng\nTian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide,\nGabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 202,
    "content": "Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,\nHamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison\nRudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 203,
    "content": "Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake\nWeissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya,\nJeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 204,
    "content": "Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan\nMcPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena,\nKartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 205,
    "content": "Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin\nChen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo,\nLicheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 206,
    "content": "Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal\nValko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 207,
    "content": "Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari,\nMunish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa,\nNayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 208,
    "content": "Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem\nKalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 209,
    "content": "Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin\nBattey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 210,
    "content": "Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru\nPan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun\nLindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 211,
    "content": "Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji\nSajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 212,
    "content": "Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara\nBest, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy\nChou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 213,
    "content": "Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,\nVladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable,\nXiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 214,
    "content": "Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin\nNam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary\nDeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 215,
    "content": "12\n\n\n-----\n\nPre p rint. Under review.\n\n[llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.](https://arxiv.org/abs/2407.21783)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 216,
    "content": "Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural\nnetworks with pruning, trained quantization and huffman coding, 2016. URL [https:](https://arxiv.org/abs/1510.00149)\n[//arxiv.org/abs/1510.00149.](https://arxiv.org/abs/1510.00149)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 217,
    "content": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with con[trastive learning, 2022. URL https://arxiv.org/abs/2112.09118.](https://arxiv.org/abs/2112.09118)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 218,
    "content": "Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O. Arik. Long-context llms meet rag:\nOvercoming challenges for long inputs in rag, 2024a. URL [https://arxiv.org/abs/2410.](https://arxiv.org/abs/2410.05983)\n[05983.](https://arxiv.org/abs/2410.05983)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 219,
    "content": "Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin.\nRagcache: Efficient knowledge caching for retrieval-augmented generation. *arXiv preprint*\n*arXiv:2404.12457*, 2024b.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 220,
    "content": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension, 2017. URL\n[https://arxiv.org/abs/1705.03551.](https://arxiv.org/abs/1705.03551)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 221,
    "content": "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models, 2020. URL\n[https://arxiv.org/abs/1911.00172.](https://arxiv.org/abs/1911.00172)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 222,
    "content": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 223,
    "content": "Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering\nresearch. *Transactions of the Association for Computational Linguistics*, 7:452–466, 2019. doi:",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 224,
    "content": "[10.1162/tacl a 00276. URL https://aclanthology.org/Q19-1026/.](https://aclanthology.org/Q19-1026/)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 225,
    "content": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rockt ¨ aschel, Sebastian Riedel, and ¨\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 226,
    "content": "[URL https://arxiv.org/abs/2005.11401.](https://arxiv.org/abs/2005.11401)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 227,
    "content": "Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight\nquantization for llm compression and acceleration, 2024. URL [https://arxiv.org/abs/](https://arxiv.org/abs/2306.00978)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 228,
    "content": "[2306.00978.](https://arxiv.org/abs/2306.00978)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 229,
    "content": "Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai\nDu, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari\nHoltzman, and Junchen Jiang. Cachegen: Kv cache compression and streaming for fast",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 230,
    "content": "[large language model serving. 2024. URL https://arxiv.org/abs/2310.07240.](https://arxiv.org/abs/2310.07240)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 231,
    "content": "Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for\n[multi-stage text retrieval, 2023. URL https://arxiv.org/abs/2310.08319.](https://arxiv.org/abs/2310.08319)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 232,
    "content": "Yu. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor\nsearch using hierarchical navigable small world graphs, 2018. URL [https://arxiv.org/](https://arxiv.org/abs/1603.09320)\n[abs/1603.09320.](https://arxiv.org/abs/1603.09320)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 233,
    "content": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh\nHajishirzi. When not to trust language models: Investigating effectiveness of parametric\n[and non-parametric memories, 2023. URL https://arxiv.org/abs/2212.10511.](https://arxiv.org/abs/2212.10511)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 234,
    "content": "Beijing Academy of Artificial Intelligence (BAAI). Baai general embedding (bge) reranker\nv2 gemma. [https://huggingface.co/BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma), 2025a. Accessed:\nMarch 29, 2025.\n\n13\n\n\n-----\n\nPre p rint. Under review.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 235,
    "content": "13\n\n\n-----\n\nPre p rint. Under review.\n\nBeijing Academy of Artificial Intelligence (BAAI). Baai general embedding (bge) reranker\nv2 m3. [https://huggingface.co/BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3), 2025b. Accessed: March 29,\n2025.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 236,
    "content": "Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. RankZephyr: Effective and\nrobust zero-shot listwise reranking is a breeze! *arXiv:2312.02724*, 2023.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 237,
    "content": "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin LeytonBrown, and Yoav Shoham. In-context retrieval-augmented language models, 2023. URL\n[https://arxiv.org/abs/2302.00083.](https://arxiv.org/abs/2302.00083)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 238,
    "content": "Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language*",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 239,
    "content": "*Processing* . Association for Computational Linguistics, 11 2019. URL [https://arxiv.org/](https://arxiv.org/abs/1908.10084)\n[abs/1908.10084.](https://arxiv.org/abs/1908.10084)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 240,
    "content": "Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25\nand beyond. *Foundations and Trends in Information Retrieval*, 3:333–389, 01 2009. doi:\n10.1561/1500000019.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 241,
    "content": "Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with a trillion-token\n[datastore, 2024. URL https://arxiv.org/abs/2407.12854.](https://arxiv.org/abs/2407.12854)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 242,
    "content": "Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training,\n[2024. URL https://arxiv.org/abs/2212.03533.](https://arxiv.org/abs/2212.03533)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 243,
    "content": "Zhepei Wei, Wei-Lin Chen, and Yu Meng. Instructrag: Instructing retrieval-augmented gen[eration via self-synthesized rationales, 2025. URL https://arxiv.org/abs/2406.13629.](https://arxiv.org/abs/2406.13629)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 244,
    "content": "Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.\nSmoothquant: Accurate and efficient post-training quantization for large language mod[els, 2024. URL https://arxiv.org/abs/2211.10438.](https://arxiv.org/abs/2211.10438)",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 245,
    "content": "Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du,\nShan Lu, and Junchen Jiang. Cacheblend: Fast large language model serving with cached\nknowledge fusion. *arXiv preprint arXiv:2405.16444*, 2024.\n### **A Prompt Template**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 246,
    "content": "Following Jin et al. (2024a)’s work, we utilize the prompt template for Generation Without\nRAG as follows:\n\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 247,
    "content": "Answer the question based on your own knowledge. Only give me the answer\nand do not output any other words.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nThe prompt template used for RAG is shown below:",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 248,
    "content": "The prompt template used for RAG is shown below:\n\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nAnswer the question based on the given document. Only give me the answer\nand do not output any other words.\nThe following are given documents.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 249,
    "content": "Doc {doc_id} (Title: {doc_title}) {doc_text}\n\n14\n\n\n-----\n\nPre p rint. Under review.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 250,
    "content": "14\n\n\n-----\n\nPre p rint. Under review.\n\nDoc {doc_id} (Title: {doc_title}) {doc_text}\nDoc {doc_id} (Title: {doc_title}) {doc_text}\nDoc {doc_id} (Title: {doc_title}) {doc_text}\nDoc {doc_id} (Title: {doc_title}) {doc_text}\nDoc {doc_id} (Title: {doc_title}) {doc_text}",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 251,
    "content": "<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nQuestion: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n### **B Quantization**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 252,
    "content": "To maintain compatibility with downstream operations (e.g., softmax), the KV cache is dequantized back to 16-bit precision after loading into GPU memory. This approach allows us\nto reduce I/O volume without introducing significant overhead during inference. However,",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 253,
    "content": "in the HyperRAG we do not apply quantization because of two reasons. The first reason is\naround the downstream performance degradation. The second reason is that the original\nKV-cache of reranker model like Gemma-2B reranker is small enough. The introduction of",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 254,
    "content": "quantization will introduce worse bandwidth performance.\n### **C Reranker Finetune**",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 255,
    "content": "To improve reranking quality in our pipeline, we fine-tune all reranker models using a\nsimple yet effective strategy: formatting the input as [document] + [query] rather than\nthe original [query] + [document] format. This reversed input order aligns better with the",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 256,
    "content": "decoder-based reranker architecture, which benefits from having the query appear later in\nthe sequence—allowing it to attend over the full document context.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 257,
    "content": "We use the open-source FlagEmbedding repository [1] as our fine-tuning framework. It supports a wide range of reranker backbones and provides efficient training tools. The rerankers\nare trained on the BGE-M3 training dataset, which contains multi-granularity positive and",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 258,
    "content": "negative pairs curated for passage and document-level retrieval tasks. We follow the standard contrastive training setup, using positive and hard negative document pairs with a\nmaximum sequence length of 512 tokens.",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  },
  {
    "chunk_id": 259,
    "content": "This fine-tuning strategy not only improves reranking performance but also makes the input\nformat compatible with our static attention layout for KV cache reuse.\n\n1 [https://github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)\n\n15\n\n\n-----",
    "metadata": {
      "source": "2504.02921v1.md"
    }
  }
]