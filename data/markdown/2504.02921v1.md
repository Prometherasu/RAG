Pre p rint. Under review.
## **HyperRAG: Enhancing Quality-Efficiency Tradeoffs in** **Retrieval-Augmented Generation with Reranker KV-Cache** **Reuse**

**Yuwei An** [1], **Yihua Cheng** [2], **Seo Jin Park** [3], **Junchen Jiang** [2]

1 Carnegie Mellon University
2 University of Chicago
3 University of Southern California
### **Abstract**

Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm for enhancing the performance of large language models (LLMs)
by integrating external knowledge into the generation process. A key component of RAG pipelines is the reranker, which selects the most relevant
documents from a pool of retrieved candidates and significantly improves
the quality of the generated responses. While rerankers refine the selection
of retrieved documents in RAG pipelines, they introduce computational
challenges that hinder high throughput and low latency. To address this
problem, we propose HyperRAG, a system that optimizes the trade-off between quality and efficiency in RAG pipelines by leveraging KV-cache reuse
for efficient reranker inference. By reusing document-side KV-cache, HyperRAG achieves both high-quality generation and system-level efficiency.
To fully realize the benefits of KV-cache reuse, HyperRAG incorporates a
range of system-level optimizations designed to enhance efficiency and
scalability. Experiments show that HyperRAG achieves a 2–3× throughput
improvement with decoder-only rerankers while also delivering higher
downstream performance compared with traditional RAG service.
### **1 Introduction**

Retrieval-Augmented Generation (RAG) (Gao et al., 2024) has emerged as a powerful
paradigm that enhances the performance of large language models (LLMs) by incorporating
external knowledge into the generation process. RAG systems typically follow a two-stage
pipeline: retrieval and generation. Among these, the retrieval stage plays a critical role as
the relevance and quality of the retrieved documents significantly influence the final output.
Selecting the most appropriate documents from a large corpus determines how effectively
the model can respond to queries, especially in open-domain settings.

Compared with the coarse-grained retrieval which is based on dense embedding similarity
over the vector index (Izacard et al., 2022; Chen et al., 2024), the reranker performs finegrained selection by scoring candidates with richer contextual understanding, leading
to more relevant and concise results (Reimers & Gurevych, 2019; Gao & Callan, 2022).
However, the reranking step introduces additional computational overhead and latency
which can become a bottleneck in high-throughput RAG systems. This issue is further
exacerbated by the adoption of large language model (LLM)-based rerankers (Chen et al.,
2024; Ma et al., 2023; Pradeep et al., 2023). On one hand, these rerankers are fine-tuned from
powerful pre-trained generative models and achieve state-of-the-art performance on various
retrieval tasks, such as passage ranking and document selection. On the other hand, they
are computationally intensive and introduce substantial latency, especially when handling
large batches of queries. As a result, despite their strong performance, LLM-based rerankers
are often impractical for real-time RAG applications.

1


-----

Pre p rint. Under review.

So the core challenge we aim to address is:

*How can we optimize the trade-off between generation quality and efficiency in RAG systems,*
*especially with large-scale LLM-based rerankers, to deliver effective results without significantly*
*compromising system throughput?*

To address this challenge, we propose HyperRAG, a system design that leverages efficient
KV-cache management to optimize the quality-efficiency trade-off in the RAG system.
Specifically, HyperRAG introduces a KV-cache storage mechanism that stores the KV-cache
of all document chunks. When performing query and document reranking, HyperRAG
efficiently loads the precomputed KV-cache, eliminating the need for recomputation and
reducing latency while maintaining high generation quality. This mechanism shifts the
bottleneck from GPU computation to SSD storage and transfer bandwidth, effectively
balancing the workload and enabling more efficient utilization of system resources.

The main contribution of HyperRAG includes:

- **Highlighting the potential of decoder based reranking** : HyperRAG emphasizes
the critical importance of rerankers in improving generation quality within RAG
pipelines. Our empirical findings show that incorporating powerful decoder-based
rerankers significantly enhances downstream performance, revealing the untapped
potential of reranking in practical RAG deployments.

- **Enabling efficient decoder-based reranking via KV-cache reuse** : To address the
computational inefficiency problem associated with decoder-based rerankers, HyperRAG introduces a KV-cache reuse mechanism. By caching the document-side
key/value pairs, the reranker only needs to process the query portion during inference. This shifts the bottleneck from GPU compute to underutilized resources such
as NVM storage and PCIe bandwidth, enabling high-throughput inference while
maintaining strong generation quality.

Our experiments demonstrate that HyperRAG maintains high-quality generation while
delivering a 2–3× throughput improvement during RAG service. This design paves the way
for a new pattern for deploying RAG systems.
### **2 Background**

In this section, we introduce the background and related work underlying the design of
HyperRAG.

**2.1** **Retrieval-Augmented Generation**

Retrieval-Augmented Generation (RAG) (Ram et al., 2023; Lewis et al., 2021; Asai et al.,
2023; Khandelwal et al., 2020; Jin et al., 2024a; Shao et al., 2024) is the process of optimizing
the output of a large language model and it references an authoritative knowledge base
outside of its training data sources before generating a response. This paradigm has gained
traction for tasks requiring factual accuracy and up-to-date information, such as question
answering, summarization, and dialogue generation.

Traditional RAG pipelines rely on retrievers
which depend on embedding representations and cosine similarity to fetch relevant
documents (Robertson & Zaragoza, 2009;
Reimers & Gurevych, 2019; Wang et al.,
2024). While this approach is straightforward, it often struggles to achieve optimal results in more complex scenarios. To

Figure 1: Classic RAG Workflow: The query

solve the problem, advancements have in
is embedded and used to retrieve top-K doc
troduced reranker mechanisms that refine

uments. Then the reranker selects the most

the retrieved documents to improve rele
relevant ones which are combined with the

vance and contextuality before generation.

query to generate the final response.

2


-----

Pre p rint. Under review.

(a) TriviaQA (b) NaturalQA (c) PopQA

Figure 2: RAG downstream performance with different rerankers: Subfigures a, b, and c
show the performance curves of exact match (EM) scores on TriviaQA (Joshi et al., 2017), NaturalQA (Kwiatkowski et al., 2019), and PopQA (Mallen et al., 2023) with various rerankers.
The x-axis denotes the number of retrieved documents involved during reranking from
which the top-1 document is selected for generation. **D-BOUND** represents the performance
upper bound is based on the number of documents during rerank while **M-BOUND** reflects
that the upper bound is determined by the reranker’s ability to identify the most relevant
document. The generation model is meta-llama/Llama-3.1-8B-Instruct . The five labels
represent different configurations: Baseline (No RAG), Embedding-only (retrieves the top
document directly using cosine similarity), E/MINILM (uses the ms-marco-MiniLM-L6v2 reranker which is Encoder-only (Face, 2025)), E/BGEM (uses the bge-reranker-v2-m3
reranker which is Encoder-only (of Artificial Intelligence, BAAI)), and D/GEMMA (uses
the Gemma 2B reranker which is Decoder-only (of Artificial Intelligence, BAAI)).

These rerankers, often transformer-based,
significantly boost the quality of generated content by ensuring the most pertinent documents are prioritized.

Early Rerankers were predominantly trained on encoder-only models such as BERT (Devlin
et al., 2019) or XLM-RoBERTa (Conneau et al., 2020), leveraging their strong encoding capabilities to improve retrieval precision. However, recent advancements have demonstrated
the growing dominance of decoder-based rerankers which capitalize on the powerful generative language capabilities of modern decoder models (Chen et al., 2024; Ma et al., 2023;
Pradeep et al., 2023). By fine-tuning these decoder-based models on tasks originally designed for encoder-only rerankers, they achieve significant gains in performance, benefiting
from both their inherent generative power and the fine-grained contextual understanding
acquired during fine-tuning.

**2.2** **KV-cache Reuse of LLM**

In generation scenario, KV-cache reuse has emerged as a vital optimization technique for
improving the efficiency of large language models (LLMs) during inference. (Liu et al.,
2024). In RAG scenarios, prior works mostly focus on reusing the KV-cache of the retrieved
documents to speed up the generation stage (Yao et al., 2024; Gim et al., 2024; Jin et al.,
2024b). In this work, we focus on reusing KV-cache during the retrieval stage to speed up
the rerank process.
### **3 Observation**

In this section, we highlight the key observations that inspired the design of HyperRAG.
In Section 3.2, we present the quality-efficiency trade-off curve inherent in RAG serving
systems which our design aims to address. This trade-off is primarily influenced by the
computational cost of embedding retrieval and reranking. In Section 3.3, we explore the
benefits of KV cache reuse during reranking inference and explain why reranking is the
optimal scenario for leveraging KV cache reuse.

3


-----

Pre p rint. Under review.

(a) Latency vs EM (b) Full vs Reuse (c) Memory Footprint (d) Throughput vs batch

Figure 3: Efficiency Observations for the Reranker: Subfigure 3(a) illustrates the trade-off between latency and NaturalQA performance across different reranker models. Subfigure 3(b)
presents the latency and throughput of the Gemma-2B reranker under varying document
chunk sizes, with the query chunk size fixed at 48. The blue line indicates full computation,
while the orange line represents computation with KV-cache reuse. Solid lines denote
latency, and dashed lines denote throughput. Subfigure 3(c) shows the memory footprint
of the Gemma-2B reranker during inference with different batch sizes, using a fixed input
length of 256 + 48 = 304. Subfigure 3(d) highlights how throughput increases with larger
batch sizes up until an out-of-memory (OOM) error occurs for Gemma-2B reranker model
inference.

**3.1** **Reranker Empowers RAG**

In retrieval-augmented generation (RAG), the quality of the retrieved documents directly
influences the final generation output. However, the initial retrieval step often returns a set
of candidate documents with mixed relevance. A reranker plays a critical role in reordering
these candidates to identify the most relevant ones, enabling the model to generate more
accurate and grounded responses.

Our findings show that a powerful reranker is essential to fully realize the potential of
RAG. Without it, even high-capacity language models struggle to make effective use of
the retrieved context. As shown in Figure 2, increasing the number of candidates reranked
improves the performance ceiling (D-BOUND), but only a strong reranker can consistently
select the best document and push the system closer to the model’s generation upper bound
(M-BOUND). This observation highlights the reranker’s pivotal role in bridging retrieval
and generation and demonstrates that a RAG system can fully realize its potential only
when equipped with a powerful reranker.

**3.2** **Quality-Efficiency Trade-off**

While larger rerankers deliver better generation quality, they are typically built on large
language models and incur significantly higher latency. As shown in Figure 3(a), more powerful rerankers such as Gemma-2B reranker model achieve higher EM scores on NaturalQA
but at a much greater computational cost compared to lighter-weight encoder models.

This trade-off becomes particularly problematic in real-world deployment scenarios. For
each incoming query, the reranker must evaluate all top- *k* retrieved documents by pairing the
query with each candidate and processing them through the model. To improve throughput,
multiple queries are typically batched together within the RAG service pipeline, resulting in
exceptionally large batch sizes. Under these conditions, rerankers based on large language
models suffer from severe performance degradation compared to their smaller counterparts.
This efficiency bottleneck is a key reason why many production RAG systems avoid using
these more powerful rerankers despite their superior retrieval quality.

**3.3** **KV-Cache Reuse**

Based on the above observation, the key challenge is to leverage powerful reranker models
while mitigating the service efficiency degradation they introduce. Given the substantial
computational overhead of large language model inference, reusing the KV-cache has
emerged as a promising strategy for accelerating decoder-based models. While prior work

4


-----

Pre p rint. Under review.

has primarily focused on applying KV-cache reuse during the generation phase, we argue
that extending this technique to the reranking stage is both more efficient and better aligned
with its characteristics.

In this section, we outline several important **properties** of reranking that make it suitable
for KV-cache reuse, and highlight the **potential** benefits this approach can bring in terms of
efficiency improvements.

**Property 1: Lossless** Compared to encoder-based rerankers, decoder-based rerankers benefit
from the tri-mask mechanism, which enables lossless two-stage inference. Specifically, this
mechanism ensures that computing the score for a ”document + query” pair yields the same
result as first performing inference on the document alone and then processing the query
using the precomputed KV-cache from the document.

Another key advantage is that reranker inference operates on a document-query pair basis,
evaluating one document chunk against a query at a time. This pairwise structure eliminates
a major challenge present in KV-cache reuse during the generation phase of RAG where
different orderings of documents (e.g., *Document A + Document B + Query* vs. *Document B +*
*Document A + Query* ) result in different KV-cache and necessitate costly recomputation. In
contrast, reranking avoids this issue entirely since each document is scored independently
with respect to the query.

**Property 2: Static** The chunk size of the document is often fixed, which brings significant
benefits for KV-cache management and inference compilation. By maintaining a consistent
chunk size, the KV-cache structure becomes predictable which simplifies memory allocation
and reduce unnecessary padding.

**Property 3: Large Reuse Ratio** In the reranking scenario, the token length of document
chunks is typically larger than that of the query. This results in a high reuse ratio, which
significantly reduces latency since the majority of tokens can be reused without recalculation.

These favorable properties make reranking an especially suitable scenario for applying
KV-cache reuse. Our observations further reveal the untapped potential of this approach to
significantly improve efficiency without compromising accuracy.

**Potential 1: Less Computation** Thanks to the high reuse ratio enabled by KV-cache reuse,
reranking with decoder-based models can significantly reduce redundant computation.
Instead of recomputing document representations for each query, the cached document-side
KV states can be reused across different document–query pairs. Figure 3(b) demonstrates
this effect using the Gemma-2B reranker. As chunk size increases, the latency of full inference
grows rapidly due to the increasing computation required for processing longer sequences.
In contrast, with KV-cache reuse, latency remains nearly constant and substantially lower
across all chunk sizes. This indicates that the document-side computation—normally the
dominant contributor to latency—has been effectively amortized. Naturally, the throughput
of reranking also improves as a result of the reduced latency.

**Potential 2: Lower Memory Footprint Enables Larger Batches** KV-cache reuse also offers
substantial memory savings, which can translate into significant throughput improvements.
Figure 3(c) shows the memory footprint of the prefill stage when running the Gemma-2B
reranker. As the chunk size increases, the intermediate activations during full prefilling
become large and quickly exhaust available GPU memory. This limits the maximum batch
size that can be processed concurrently. Figure 3(d) demonstrates that batch size can be a key
bottleneck for throughput—when memory is constrained, which means that the memory
upper bound comes first compared to compuation upper bound.

In contrast, KV-cache reuse significantly reduces the intermediate memory requirements
by skipping document-side prefilling during inference. With reduced memory usage, the
reranker can support higher levels of parallelism, enabling larger batch sizes and increased
throughput during reranking.

5


-----

Pre p rint. Under review.
### **4 HyperRAG**


Figure 4: Overview of HyperRAG


In this section, we present the overall system design of **HyperRAG**, our high-performance
retrieval-augmented generation (RAG) framework optimized for both efficiency and scalability. The rest of this section is organized as follows. Section 4.1 introduces the design and
benefits of KV-cache compression in HyperRAG. Sections 4.2, 4.3, and 4.4 then discuss the
hierarchical design of HyperRAG, moving from GPU to CPU and finally to Storage.

**4.1** **KV-cache Compression: Addressing the Storage Bottleneck**

While KV-cache reuse offers significant latency and throughput benefits, it introduces a new
set of challenges, particularly when scaling to large datastores. In this scenario, the system
may need to offload KV-cache to external storage devices such as NVMe or cloud-based
object stores. As a result, the bottleneck shifts from GPU computation to the storage and data
transfer layers. Two major issues emerge: the overall size of the KV-cache and the bandwidth
required to transfer it between storage and GPU memory. To address this, we propose
lightweight KV-cache compression techniques that are critical for scaling HyperRAG to
large deployments.

**Efficient Attention Mechanisms:** Compared to traditional attention mechanisms, recent
designs such as Grouped Query Attention (GQA) (Ainslie et al., 2023) and Multi-Latent
Attention (MLA) (DeepSeek-AI et al., 2024) significantly reduce KV-cache memory requirements per token. While originally developed to improve GPU decoding throughput, these
mechanisms are particularly beneficial in the context of HyperRAG. Since reranker inference
involves transferring KV-cache for every document–query pair, reducing the per-token KV
footprint directly eases pressure on storage bandwidth and memory usage. Among the
rerankers we evaluated, the Gemma-2B reranker is especially well-suited for this setting
due to its low KV memory cost per token, making it ideal for large-scale and cache-intensive
RAG inference.

**KV-cache Quantization:** Quantization (Han et al., 2016; Lin et al., 2024; Xiao et al., 2024)
is a widely-used technique for compressing model weights and activations by reducing
precision. In HyperRAG, we extend this concept to KV-cache, focusing on minimizing
storage and transfer cost while preserving model performance. Rather than applying full
6


-----

Pre p rint. Under review.

model quantization schemes like **W8A8** or **W4A16**, we adopt specialized formats such as
**KV8W16A16** and **KV4W16A16**, where quantization is applied exclusively to KV-cache
while keeping model weights and activations in higher precision. However in the following
experiment we do not use quantization further. Appendix B shows the reason why we do
not use it for Gemma-2B reranker.

**4.2** **GPU-Centric Computation**

In HyperRAG, GPU resources are responsible for two main computational roles: (1) retrievalside processing—including embedding generation and reranking—and (2) generation-side
inference, where the top-ranked document is combined with the query to produce the final
response.

To maximize system throughput, it is crucial to balance the computational load between these two stages. If either the reranking or generation side becomes a bottleneck,
overall throughput suffers due to underutilized GPU resources. HyperRAG addresses
this by disaggregating retrieval and gen- Figure 5: Static KV Layout: During reranking,
eration pipelines and carefully managing we allocate a fixed-length KV buffer for attenGPU allocation. By dynamically tuning the tion. The buffer consists of a static document
number of GPUs assigned to each stage and segment (retrieved KV, shown in red) and a
leveraging KV-cache reuse to reduce redun- static query segment (computed KV, shown in
dant computation, HyperRAG maintains a green).
balanced and efficient end-to-end inference
pipeline.

In addition to load balancing, HyperRAG further optimizes GPU execution by adopting
a **static attention layout**, enabling better fusion and graph capture through torch compile
and CUDA graphs (Ansel et al., 2024). As illustrated in Figure 5, we fix the structure of the
attention kv-input such that the document occupies a static prefix region while the query
occupies a fixed-length suffix. The document tokens use precomputed KV-cache (retrieved
from memory) and only the query portion contributes to new KV computation. This static
partitioning allows the entire decoding process to be compiled ahead of time and executed
with minimal runtime overhead, greatly improving inference efficiency.

**4.3** **CPU-Centric Index**

In the Embedding retrieval stage of HyperRAG, we leverage FAISS, a highly optimized
library for efficient similarity search on dense vectors. This CPU-centric approach ensures
the rapid indexing and retrieval of top-K documents. FAISS operates on the dense embeddings generated by the model, implementing approximate nearest neighbor (ANN) search
algorithms that are both scalable and precise. Specifically, we employ IVF (Inverted File Index) and HNSW (Hierarchical Navigable Small World) (Malkov & Yashunin, 2018) indexing
structures within FAISS (Douze et al., 2025) which enables fast and memory-efficient search
while maintaining high recall.

**4.4** **Storage Backend**

To support efficient and scalable KV-cache access in HyperRAG, we build our storage
backend on top of the open-source *LMCache* framework (Cheng et al., 2024). This backend
is designed to provide near-constant KV-cache retrieval latency, even as the total cache size
scales to billions of tokens. It offers fast and reliable KV-cache management across multiple
storage backends, such as local NVMe drives and remote systems like Redis.

A key feature of our storage layer is its ability to seamlessly partition the KV-cache across
multiple storage devices, such as independent NVMe drives or remote object storage.
Given the scale of KV-cache required for large datastores, such partitioning is essential.
To maintain retrieval efficiency in this setting, we align the partitioning strategy with the

7


-----

Pre p rint. Under review.

**BGE-M3** **Gemma** **H-Gemma**
C Metrics








Table 1: Downstream Performance Evaluation: We report Exact Match (EM) scores on
three QA benchmarks along with throughput across different settings. The column labeled
C denotes the corpus type: Passage-level (P) or Document-level (D). For each reranker
model (BGE-M3 (of Artificial Intelligence, BAAI), Gemma (of Artificial Intelligence, BAAI),
HyperRAG Gemma(H-Gemma)), we vary the number of retrieved documents involved in
reranking (shown as 5, 10, and 20).

indexing structure of the embedding retriever. Specifically, we group KV-cache entries by
the *coarse centroid ID* used in the FAISS IVF index. This means that all KV entries assigned
to the same coarse cluster are stored within the same storage backend.

This design offers a critical performance benefit: since approximate nearest neighbor (ANN)
search during retrieval only probes a small number of centroid clusters, the corresponding
KV-cache retrieval is limited to a single backend. As a result, we avoid cross-device fetching,
reduce latency, and improve I/O locality.
### **5 Experiment**

In this section, we present the results of the benchmark and performance experiments.

**5.1** **Experiment settings**

**Corpus** :We evaluate HyperRAG under two different levels of retrieval granularity: passagelevel and document-level.

*Passage-Level Corpus:* For the passage-level setting, we use the psgs multiset-100 dataset as
our base corpus. To ensure consistent input lengths and improve KV cache reuse efficiency,
we rechunk all passages to a fixed size of approximately 200 words per chunk. In this case
document tokens length is fixed as 256.

*Document-Level Corpus:* For the document-level setting, we adopt the MS MARCO (Bajaj
et al., 2018) document dataset. Each document is chunked into fixed-length segments with a
maximum word length of 450. In this case document tokens length is fixed as 512.

**Metrics** : To evaluate downstream performance, we utilized TriviaQA (Joshi et al., 2017),
NaturalQA (Kwiatkowski et al., 2019), and PopQA (Mallen et al., 2023) as benchmarks
for performance evaluation. For the benchmark measurement, we collect the average
throughput of RAG service(request per second) after warm-up for efficiency evaluation.

**Model** : For the embedding model of index search, we used the contriever (Izacard et al.,
2022) model. In the generation stage, we employed the Llama3.1-8B-Instruct model
(Grattafiori et al., 2024).

**Hardware** The experiments were conducted on a system equipped with 8 NVIDIA A100
GPUs. For storage backend we use redis remote backend(4TB) and local nvme backend(4TB).
We store all the hot KV-cache that will be used during inference on the QA dataset and then
fulfill the whole storage with random sampling.

8


-----

Pre p rint. Under review.

**5.2** **Trade-off Performance**

We conduct experiments to evaluate both downstream performance and system efficiency.
The results, presented in Table 1, demonstrate that HyperRAG maintains high-quality
generation while achieving a 2–3× improvement in service throughput on average. These
findings highlight HyperRAG’s ability to effectively balance generation quality and costefficiency, making it a robust and practical solution for RAG service.
### **6 Discussion**

In this section we will provide some key discussions around HyperRAG.

**6.1** **RAG Service Framework**

One of the recent hot topics in RAG research is enabling the generation model to better
identify and utilize the most relevant retrieved documents (Asai et al., 2023; Wei et al.,
2025). HyperRAG aligns with this research direction, but with a key distinction: instead of
relying solely on the generation model to perform this filtering, we delegate the relevance
discrimination task to a powerful reranker. This reranker operates with similar mechanisms
to large language models but is significantly more efficient due to its smaller size and the
support of HyperRAG’s architecture. Moreover, this separation of relevance extraction
offers greater flexibility during deployment.

**6.2** **Real-World Application**

Storing reranker KV caches introduces a significant storage requirement. For example, we
estimate that storing the full KV cache for the Gemma-2B reranker over the MS MARCO
document dataset would require more than 40TB of storage. This raises questions about
the practicality of deploying HyperRAG in real-world settings. However, we argue that
HyperRAG remains a viable and efficient solution for two main reasons.

**Production Stack Design:** Figure 6 illustrates a potential distributed deployment
architecture for HyperRAG. In this setup,
the KV cache backend functions as a shared
memory store accessible by multiple RAG
service engines. This design allows the storage burden to be amortized across multiple
applications and services, enabling efficient
and centralized knowledge sharing at scale.

**Financial Cost:** In practice, storage costs are
negligible compared to GPU infrastructure. Figure 6: Production Stack: The stored KV
For instance, on Amazon Web Services, the cache backend can be shared across multiple
cost of provisioning 8×A100 GPUs far ex- RAG service engines.
ceeds that of acquiring 40TB of persistent
storage. By introducing a relatively small additional cost for storage, HyperRAG achieves
significantly higher throughput—making it a more cost-effective solution from the perspective of service providers aiming to optimize performance per dollar.
### **References**

Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, ´
and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from
[multi-head checkpoints, 2023. URL https://arxiv.org/abs/2305.13245.](https://arxiv.org/abs/2305.13245)

Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali
Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng,

9


-----

Pre p rint. Under review.

Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar,
Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu,
C. K. Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim,
Marcos Yukio Siraichi, Helen Suk, Shunting Zhang, Michael Suo, Phil Tillet, Xu Zhao,
Eikan Wang, Keren Zhou, Richard Zou, Xiaodong Wang, Ajit Mathews, William Wen,
Gregory Chanan, Peng Wu, and Soumith Chintala. Pytorch 2: Faster machine learning
through dynamic python bytecode transformation and graph compilation. In *Proceedings*
*of the 29th ACM International Conference on Architectural Support for Programming Languages*
*and Operating Systems, Volume 2*, ASPLOS ’24, pp. 929–947, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703850. doi: 10.1145/3620665.3640366.
[URL https://doi.org/10.1145/3620665.3640366.](https://doi.org/10.1145/3620665.3640366)

Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag:
Learning to retrieve, generate, and critique through self-reflection, 2023. URL [https:](https://arxiv.org/abs/2310.11511)
[//arxiv.org/abs/2310.11511.](https://arxiv.org/abs/2310.11511)

Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan
Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song,
Alina Stoica, Saurabh Tiwary, and Tong Wang. Ms marco: A human generated machine
[reading comprehension dataset, 2018. URL https://arxiv.org/abs/1611.09268.](https://arxiv.org/abs/1611.09268)

Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge
m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings
[through self-knowledge distillation, 2024. URL https://arxiv.org/abs/2402.03216.](https://arxiv.org/abs/2402.03216)

Yihua Cheng, Kuntai Du, Jiayi Yao, and Junchen Jiang. Do large language models need a
content delivery network? *arXiv preprint arXiv:2409.13761*, 2024.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin ´
Stoyanov. Unsupervised cross-lingual representation learning at scale, 2020. URL [https:](https://arxiv.org/abs/1911.02116)
[//arxiv.org/abs/1911.02116.](https://arxiv.org/abs/1911.02116)

DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao,
Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji,
Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang,
Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui
Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang
Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong
Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan
Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi
Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge,
Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang
Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping
Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao,
Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q.
Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen,
Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu,
Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su,
Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao,
Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong
Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang,
Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma,
Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen
Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu,
Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseekv2: A strong, economical, and efficient mixture-of-experts language model, 2024. URL
[https://arxiv.org/abs/2405.04434.](https://arxiv.org/abs/2405.04434)

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding, 2019. URL [https://arxiv.](https://arxiv.org/abs/1810.04805)
[org/abs/1810.04805.](https://arxiv.org/abs/1810.04805)

10


-----

Pre p rint. Under review.

Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herv ´ e J ´ egou. The faiss library, ´
[2025. URL https://arxiv.org/abs/2401.08281.](https://arxiv.org/abs/2401.08281)

Hugging Face. cross-encoder/ms-marco-minilm-l6-v2. [https://huggingface.co/](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2)
[cross-encoder/ms-marco-MiniLM-L6-v2, 2025. Accessed: March 29, 2025.](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2)

Luyu Gao and Jamie Callan. Long document re-ranking with modular re-ranker. In
*Proceedings of the 45th International ACM SIGIR Conference on Research and Development in*
*Information Retrieval*, SIGIR ’22, pp. 2371–2376. ACM, July 2022. doi: 10.1145/3477495.
[3531860. URL http://dx.doi.org/10.1145/3477495.3531860.](http://dx.doi.org/10.1145/3477495.3531860)

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,
Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language
[models: A survey, 2024. URL https://arxiv.org/abs/2312.10997.](https://arxiv.org/abs/2312.10997)

In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong.
Prompt cache: Modular attention reuse for low-latency inference. *Proceedings of Machine*
*Learning and Systems*, 6:325–338, 2024.

Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy
Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,
Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer,
Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny
Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego
Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan,
Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, ´
Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon,
Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron,
Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack
Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar,
Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,
Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak,
Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden
Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin
Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal
Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz
Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke
de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin
Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie
Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal,
Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang,
Olivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar
Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,
Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,
Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan
Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean
Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende,
Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas
Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami,
Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish
Vogeti, V ´ ıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney
Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia,

11


-----

Pre p rint. Under review.

Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei,
Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert,
Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha
Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand,
Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda
Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew
Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani,
Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley
Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer,
Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing
Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic,
Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris
Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer,
Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana
Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa
Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik
Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng
Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide,
Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern,
Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,
Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison
Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim
Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake
Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya,
Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul,
Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan
McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena,
Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly
Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin
Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo,
Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish
Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal
Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark,
Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari,
Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa,
Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev,
Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem
Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj,
Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu
Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin
Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh
Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru
Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun
Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil,
Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji
Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk,
Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara
Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy
Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan,
Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,
Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable,
Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman,
Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin
Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary
DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The

12


-----

Pre p rint. Under review.

[llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.](https://arxiv.org/abs/2407.21783)

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural
networks with pruning, trained quantization and huffman coding, 2016. URL [https:](https://arxiv.org/abs/1510.00149)
[//arxiv.org/abs/1510.00149.](https://arxiv.org/abs/1510.00149)

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with con[trastive learning, 2022. URL https://arxiv.org/abs/2112.09118.](https://arxiv.org/abs/2112.09118)

Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O. Arik. Long-context llms meet rag:
Overcoming challenges for long inputs in rag, 2024a. URL [https://arxiv.org/abs/2410.](https://arxiv.org/abs/2410.05983)
[05983.](https://arxiv.org/abs/2410.05983)

Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin.
Ragcache: Efficient knowledge caching for retrieval-augmented generation. *arXiv preprint*
*arXiv:2404.12457*, 2024b.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large
scale distantly supervised challenge dataset for reading comprehension, 2017. URL
[https://arxiv.org/abs/1705.03551.](https://arxiv.org/abs/1705.03551)

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models, 2020. URL
[https://arxiv.org/abs/1911.00172.](https://arxiv.org/abs/1911.00172)

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina
Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering
research. *Transactions of the Association for Computational Linguistics*, 7:452–466, 2019. doi:
[10.1162/tacl a 00276. URL https://aclanthology.org/Q19-1026/.](https://aclanthology.org/Q19-1026/)

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rockt ¨ aschel, Sebastian Riedel, and ¨
Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.
[URL https://arxiv.org/abs/2005.11401.](https://arxiv.org/abs/2005.11401)

Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight
quantization for llm compression and acceleration, 2024. URL [https://arxiv.org/abs/](https://arxiv.org/abs/2306.00978)
[2306.00978.](https://arxiv.org/abs/2306.00978)

Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai
Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari
Holtzman, and Junchen Jiang. Cachegen: Kv cache compression and streaming for fast
[large language model serving. 2024. URL https://arxiv.org/abs/2310.07240.](https://arxiv.org/abs/2310.07240)

Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for
[multi-stage text retrieval, 2023. URL https://arxiv.org/abs/2310.08319.](https://arxiv.org/abs/2310.08319)

Yu. A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor
search using hierarchical navigable small world graphs, 2018. URL [https://arxiv.org/](https://arxiv.org/abs/1603.09320)
[abs/1603.09320.](https://arxiv.org/abs/1603.09320)

Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh
Hajishirzi. When not to trust language models: Investigating effectiveness of parametric
[and non-parametric memories, 2023. URL https://arxiv.org/abs/2212.10511.](https://arxiv.org/abs/2212.10511)

Beijing Academy of Artificial Intelligence (BAAI). Baai general embedding (bge) reranker
v2 gemma. [https://huggingface.co/BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma), 2025a. Accessed:
March 29, 2025.

13


-----

Pre p rint. Under review.

Beijing Academy of Artificial Intelligence (BAAI). Baai general embedding (bge) reranker
v2 m3. [https://huggingface.co/BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3), 2025b. Accessed: March 29,
2025.

Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. RankZephyr: Effective and
robust zero-shot listwise reranking is a breeze! *arXiv:2312.02724*, 2023.

Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin LeytonBrown, and Yoav Shoham. In-context retrieval-augmented language models, 2023. URL
[https://arxiv.org/abs/2302.00083.](https://arxiv.org/abs/2302.00083)

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language*
*Processing* . Association for Computational Linguistics, 11 2019. URL [https://arxiv.org/](https://arxiv.org/abs/1908.10084)
[abs/1908.10084.](https://arxiv.org/abs/1908.10084)

Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25
and beyond. *Foundations and Trends in Information Retrieval*, 3:333–389, 01 2009. doi:
10.1561/1500000019.

Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with a trillion-token
[datastore, 2024. URL https://arxiv.org/abs/2407.12854.](https://arxiv.org/abs/2407.12854)

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training,
[2024. URL https://arxiv.org/abs/2212.03533.](https://arxiv.org/abs/2212.03533)

Zhepei Wei, Wei-Lin Chen, and Yu Meng. Instructrag: Instructing retrieval-augmented gen[eration via self-synthesized rationales, 2025. URL https://arxiv.org/abs/2406.13629.](https://arxiv.org/abs/2406.13629)

Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
Smoothquant: Accurate and efficient post-training quantization for large language mod[els, 2024. URL https://arxiv.org/abs/2211.10438.](https://arxiv.org/abs/2211.10438)

Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du,
Shan Lu, and Junchen Jiang. Cacheblend: Fast large language model serving with cached
knowledge fusion. *arXiv preprint arXiv:2405.16444*, 2024.
### **A Prompt Template**

Following Jin et al. (2024a)’s work, we utilize the prompt template for Generation Without
RAG as follows:

<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Answer the question based on your own knowledge. Only give me the answer
and do not output any other words.<|eot_id|><|start_header_id|>user<|end_header_id|>

Question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

The prompt template used for RAG is shown below:

<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Answer the question based on the given document. Only give me the answer
and do not output any other words.
The following are given documents.

Doc {doc_id} (Title: {doc_title}) {doc_text}

14


-----

Pre p rint. Under review.

Doc {doc_id} (Title: {doc_title}) {doc_text}
Doc {doc_id} (Title: {doc_title}) {doc_text}
Doc {doc_id} (Title: {doc_title}) {doc_text}
Doc {doc_id} (Title: {doc_title}) {doc_text}
Doc {doc_id} (Title: {doc_title}) {doc_text}

<|eot_id|><|start_header_id|>user<|end_header_id|>

Question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
### **B Quantization**

To maintain compatibility with downstream operations (e.g., softmax), the KV cache is dequantized back to 16-bit precision after loading into GPU memory. This approach allows us
to reduce I/O volume without introducing significant overhead during inference. However,
in the HyperRAG we do not apply quantization because of two reasons. The first reason is
around the downstream performance degradation. The second reason is that the original
KV-cache of reranker model like Gemma-2B reranker is small enough. The introduction of
quantization will introduce worse bandwidth performance.
### **C Reranker Finetune**

To improve reranking quality in our pipeline, we fine-tune all reranker models using a
simple yet effective strategy: formatting the input as [document] + [query] rather than
the original [query] + [document] format. This reversed input order aligns better with the
decoder-based reranker architecture, which benefits from having the query appear later in
the sequence—allowing it to attend over the full document context.

We use the open-source FlagEmbedding repository [1] as our fine-tuning framework. It supports a wide range of reranker backbones and provides efficient training tools. The rerankers
are trained on the BGE-M3 training dataset, which contains multi-granularity positive and
negative pairs curated for passage and document-level retrieval tasks. We follow the standard contrastive training setup, using positive and hard negative document pairs with a
maximum sequence length of 512 tokens.

This fine-tuning strategy not only improves reranking performance but also makes the input
format compatible with our static attention layout for KV cache reuse.

1 [https://github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

15


-----
