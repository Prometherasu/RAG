## **Improving the Context Length and Efficiency of Code Retrieval for Tracing** **Security Vulnerability Fixes**
### XUEQING LIU, Stevens Institute of Technology, USA JIANGRUI ZHENG, Stevens Institute of Technology, USA GUANQUN YANG, Stevens Institute of Technology, USA SIYAN WEN, Stevens Institute of Technology, USA QIUSHI LIU, ZJU-UIUC Institute, China

In recent years, the rapid increase of security vulnerabilities has caused major challenges in managing them. One critical task

in vulnerability management is tracing the patches that fix a vulnerability. By accurately tracing the patching commits, security

stakeholders can precisely identify affected software components, determine vulnerable and fixed versions, assess the severity etc.,

which facilitates rapid deployment of mitigations. However, previous work has shown that the patch information is often missing

in vulnerability databases, including both the National Vulnerability Databases (NVD) and the GitHub Advisory Database, which

increases the risk of delayed mitigation, incorrect vulnerability assessment, and potential exploits.

Although existing work has proposed several approaches for patch tracing, they suffer from two major challenges: (1) the lack of

scalability to the full-repository level, and (2) the lack of study on how to model the semantic similarity between the CVE and the

full diff code. Upon identifying this gap, we propose SITPatchTracer, a scalable full-repo full-context retrieval system for security

vulnerability patch tracing. SITPatchTracerleverages ElasticSearch, learning-to-rank, and a hierarchical embedding approach based

on GritLM [ 38 ], a top-ranked LLM for text embedding with unlimited context length and fast inference speed. The evaluation of

SITPatchTracershows that it achieves a high recall on both evaluated datasets. SITPatchTracer’s recall not only outperforms

several existing works (PatchFinder [ 30 ], PatchScout [ 49 ], VFCFinder [ 14 ]), but also Voyage, the SOTA commercial code embedding

API by 13% and 28%.

Additional Key Words and Phrases: Security Vulnerability; Traceability; Open-Source Software; Information Retrieval; Large Language

Models

**ACM Reference Format:**

Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu. 2018. Improving the Context Length and Efficiency of

Code Retrieval for Tracing Security Vulnerability Fixes . *ACM/IMS J. Data Sci.* [37, 4, Article 111 (August 2018), 21 pages. https:](https://doi.org/XXXXXXX.XXXXXXX)

[//doi.org/XXXXXXX.XXXXXXX](https://doi.org/XXXXXXX.XXXXXXX)

**1** **Introduction**

With the tremendous increase in OSS security vulnerabilities, it is imperative to improve the data management for

security vulnerability information. Poor vulnerability management can pose a higher risk for OSS to be exploited by

Authors’ Contact Information: Xueqing Liu, Stevens Institute of Technology, Hoboken, NJ, USA, xliu127@stevens.edu; Jiangrui Zheng, Stevens Institute

of Technology, Hoboken, NJ, USA, jzheng36@stevens.edu; Guanqun Yang, Stevens Institute of Technology, Hoboken, NJ, USA, gyang16@stevens.edu;

Siyan Wen, Stevens Institute of Technology, Hoboken, NJ, USA, swen4@stevens.edu; Qiushi Liu, ZJU-UIUC Institute, Haining, Zhejiang, China,

qiushi3@illinois.edu.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not

made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components

of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on

servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.

Manuscript submitted to ACM

Manuscript submitted to ACM 1


-----

2 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

(a) An example CVE-2013-1814 whose patch is missing (Nov
2024) in the NVD database

(b) An Example of GitHub Maintainer’s Efforts to Find the
Patch for CVE-2013-1814

Fig. 1. An example of NVD’s missing patch link

malicious attackers, causing significant damage such as data breaches and financial losses. For example, in 2017, an

Apache Struts vulnerability enabling remote code execution (RCE) was exploited in the Equifax breach [ 56 ], leading

to an identity theft attack that affected millions of users and caused $425 million in losses. Three months before the

attack, the patch was disclosed in the National Vulnerability Database [ 2 ], indicating the attack was the result of poor

management.

One critical task in vulnerability management is tracing the commits for fixing a vulnerability [ 14, 30, 45, 48, 49, 54, 62,

64 ]. By locating the patch, security stakeholders can more accurately determine the affected version [ 6 ], identify affected

software components[ 9, 10, 21, 34 ], and improve the severity assessment [ 29 ]. A lack of accurate patch information can

increase the difficulty for security maintainers to identify the above information. For example, Figure 1 shows CVE-2013
1814, whose patch is missing in NVD. Thus when a GitHub user requests to change its affected package information to

the GitHub Advisory database [ 17 ], the GitHub maintainer has to manually search the patch by him/herself to validate

the affected package information.

Despite the importance of patch tracing, studies find that the patch information is often missing in 63% CVEs in the

GitHub Advisory Database [ 14 ]; the National Vulnerability Databases (NVD) experiences a more severe delay [ 2, 12, 22,

23 ]. To this end, previous works [ 14, 30, 45, 48, 49, 54, 62, 64 ] study how to automatically retrieve the patching commit.

Existing works leverage learning to rank[ 49, 54 ], text embedding [ 14 ], and fine-tuning language models [ 14, 30, 54 ] to

improve the model performance. Nevertheless, we identify two major challenges. First, a lack of evaluation on the full
repository scale. Multiple existing works evaluate their models by randomly sampling 5,000 negative commits [ 30, 49, 54 ].

However, our dataset shows that 38% repositories contain more than 5,000 commits. Second, there exist fewer studies

on modeling the similarity between the CVE description and the code at the full-context level. Existing work [ 14, 30, 54 ]

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 3

truncate the code to 512 tokens, causing significant information loss [1], especially when the commit message is only

briefly written [41].

Upon observing existing works’ limitations in scalability and code modeling, we propose SITPatchTracer, a scalable

and effective retrieval system for security vulnerability patch tracing. SITPatchTracerleverages the following steps to

improve the scalability and retrieval scores: (1) First, we leverage BM25 using ElasticSearch and the time difference

between a CVE and the commit to efficiently pre-rank the candidate commits; (2) Second, to reduce the truncation

of the diff code, we propose a hierarchical embedding approach based on GritLM [ 38 ], a top-ranked LLM for text

embedding with unlimited context length and fast inference speed. During indexing time, the hierarchical embedding

splits each diff into files, then index the commit message and each file’s diff. During query time, it aggregates the

file-level embeddings to obtain different similarity commit-level features. (3) To bridge the gap when the CVE description

mentions out-of-commit entities (e.g., package names, function names), we build a path similarity feature by searching

for the file paths matching these entities, and then computing their similarity with the commit paths. (4) Finally, we build

a learning-to-rank [7, 26] framework based on LightGBM’s LambdaRank algorithm [26] and the FLAML library [53].

To compare SITPatchTracer’s performance with existing work, we use the PatchFinder [ 30 ] and a dataset we

build based on GitHub Advisory database. The result shows that SITPatchTracerachieves a recall@10 of 0.736 and

0.573 respectively, which outperforms existing work on patch tracing, including PatchFinder [ 30 ], PatchScout [ 49 ], and

VFCFinder [ 14 ]. Furthermore, this recall outperforms VoyageAI [ 4 ], the top-1 ranked commercial code embedding API

for code retrieval, by 13% and 28% respectively. SITPatchTracertakes approximate 40 mins to process every 10000

commits, which is manageable for practical deployment.

In this paper, we make the following contributions:

- We propose SITPatchTracer, a scalable full-repo full-context retrieval system for security vulnerability patching

commit.

- We introduce the hierarchical embedding approach to handle the long context of the commit diff code. The

hierarchical embedding approach can be easily extended to other tasks such as code search;

- SITPatchTracerachieves high recalls, which not only outperforms existing work on patch retrieval (PatchFinder [ 30 ],

PatchScout [ 49 ], VFCFinder [ 14 ]), but also outperforms the SOTA commercial code embedding API (VoyageAI)

by 13% and 28% on our two datasets;

**2** **Challenges in Patch Retrieval**

**Challenge in Handling Large Scale Repositories** . Existing work propose several strategies to reduce the scalability

challenge. First, using a low-cost retrieval model to pre-rank candidate commits. For example, PatchFinder uses a

two-phrase framework: (1) Phase 1: pre-ranks the candidate commits using TF-IDF and the BertScore method [ 65 ], (2)

Phase 2: re-ranks the top commits using a fine-tuned model. The Phase 1 TF-IDF in PatchFinder and other existing

work [ 49, 54 ] all uses *forward indexing*, significantly limiting the scalability. Furthermore, BertScore is not an embedding

model, prohibiting PatchFinder to leverage the efficient indexing/querying pipeline of text embedding. Second, using

version tag to filter the candidate commits. VFCFinder [ 14 ] restricts the candidate to commits between the fixed version

and the prior version. While this approach can largely reduce the candidate commit size, it relies on the availability

and accuracy of the fixed version. To examine the accuracy of the fixed version, we conduct an empirical study in

Section A of the Appendix which reveals the fixed version tag only contains 60.24% of the patching commits. Without

1 Our study in Table 3 shows the average commit contains 12,000 to 19,000 tokens.

Manuscript submitted to ACM


-----

4 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

solving the scalability challenge, multiple existing works resort to evaluation on the subset of patch + 5,000 sampled

negative commits [ 30, 49, 54 ]. This unrealistic setting will significantly overestimate the evaluated methods’ accuracy

and underestimate its time cost.

**Challenge in Modeling Code** . We find that how to model the semantic similarity between CVE and the full diff code

is relatively under-explored. The existing efforts for improving the code modeling[ 14, 30, 49, 54 ] can be categorized as

feature engineering and deep learning. First, several works [ 45, 49, 54 ] focus on creating count-based features to be

applied in their learning-to-rank framework. For example, PatchScout [ 49 ] introduces 22 features including the number

of shared words, function names, etc. between the CVE description and the code [ 49 ]. Although count-based features are

easy to understand, they often have lower accuracy than deep learning models [ 30 ]. Maintaining too many handcrafted

features can also make the method hard to manage and reproduce. Second, while existing work [ 14, 30, 54 ] leverage

deep learning models (e.g., BERT [ 54 ], CodeBERT [ 14 ], BertScore + CodeReviewer [ 30 ]), they have not explicitly studied

how to handle the long diff code, which includes 12,000-19,000 tokens in average (Table 3), while the maximum input

length of BERT is 512. As a result, existing work [ 14, 30, 54 ] has to truncate the diff code to 512 tokens, leading to

significant loss of information, especially when the commit message of the patch is less informative, e.g., the commit

message for CVE-2021-41079’s patch [41] is: " *Improving robustness* ".

**3** **Approach**

**3.1** **Overview**

Following existing work [ 30 ], we propose to first pre-rank all commits in a repository using a low-cost retrieval model,

then re-rank the top results using a more expensive model. The two phases are described below.

**Pre-Ranking using TF-IDF/BM25** . For pre-ranking, we propose to implement BM25 using the inverted index, which

is a data structure that maps words to the documents they appear, enabling fast and efficient full-text searches [ 58 ]. We

use ElasticSearch [ 15 ] 7.9.0, We further leverage the time between the CVE and commit, which takes negligible time to

compute but can effectively improve the patch retrieval score [49].

**Re-Ranking using Code Embedding** . After achieving a high recall at position 10,000, we propose to use text

embedding [ 4, 32, 33, 38, 39, 43 ] to further re-rank the top-10,000 commits. We thus only index the union of the top






-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 5

The diff code in a commit typically contains 12,000 to 19,000 tokens (Table 3), which is longer than most open-source

embedding models [ 33, 38 ] can effectively handle. To reduce code truncation, we propose a hierarchical embedding

approach (Section 3.3). Our hierarchical embedding approach splits each diff code into files, indexes the commit message

and the first 512 tokens of each file, and then aggregates the file-level embedding at query time to compute different

similarity features.

**Overview of SITPatchTracerFramework** . The final SITPatchTracerframework is shown in Figure 2. It includes

the following component. First, it pre-ranks all commits in a repo using BM25+Time with ElasticSearch (Section 3.2).

Second, for the top-10000 commits, it computes the code embedding using our hierarchical embedding approach

(Section 3.3). Third, to bridge the gap when the CVE description mentions out-of-commit entities (e.g., package names,

function names), it includes a path similarity feature by searching for the file paths matching these entities, then

computing their similarity with the commit paths (Section 3.4). Finally, it leverages learning-to-rank [ 7, 26 ] (Section 3.5)

to combine all features. We summarize the features of SITPatchTracerin Table 1.

Table 1. Features used in SITPatchTracer

Feature Grou p Features

1. GritLM cosine with truncated diff
Code embedding 2. Max GritLM cosine with all files in diff
3. GritLM cosine with mean of top-1 vectors of all files in diff
4. GritLM cosine with mean of top-2 vectors of all files in diff
BM25 5. BM25 ElasticSearch

6. #commits between CVE reserve time
Time
and commit

7. #commits between CVE publish time
and commit

8. Jaccard Index between NER-paths and
Path
commit-paths
9. Voyage AI [ 4 ] cosine between NERp aths and commit- p aths

**3.2** **Pre-Ranking Large Repo using ElasticSearch and Time Distance**

We use ElasticSearch 7.9.0 [ 15 ] to pre-rank all commits in a repository using the BM25 model. To improve the recall of

BM25, we compute the similarity with each commit’s commit message and diff code separately, then aggregate them

using weighted sum. For each repository, we thus create two indices: one for commit messages and one for the diff. For

the diff, we further use the camel/snake case tokenization in ElasticSearch to more effectively handle the code.

**Using Time Distance to Improve Pre-Ranking** . To further improve the recall of BM25, we propose to leverage the

difference in the number of commits between the CVE and the commit. The intuition is that a security vulnerability

needs to be fixed before or shortly after the CVE is published, thus the time difference between the CVE and the commit

should be small. Existing work also leverage the time affinity to improve the ranking [ 49 ]. In particular, we sort all the

commits in a repository by their commit time and store the index of each commit in this list. During the query time, we

use binary search to locate the position of the CVE time in the sorted list, then we compute the number of commits

between the CVE time and each commit. This approach allows us to decouple the time difference calculation between

Manuscript submitted to ACM


-----

6 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

the CVE and the commit, achieving a significant improvement in recall with minimal overhead. For each CVE, we use

both the publish time and the reserve time (i.e., " *datePublished* " and " *dateReserved* " in the MITRE CVE database [ 1 ]).

The scores of (commit message, diff code, reserve time, and publish time) are combined using the weighted sum with

weight [0.35, 0.15, 0.3, 0.2]. To make the scores comparable, we convert each score to the reciprocal rank (i.e., 1/rank)

before combining them.

**3.3** **Encoding Full-Context Commit using Hierarchical Embedding**

**Model Selection** . To build a model for embedding the long diff code, we select from state-of-the-art text embedding

models following the massive text embedding benchmark (MTEB) [ 39 ]. In particular, we select from top models on

the CoIR [ 31 ] and CodeSearchNet [ 24 ] benchmark, which includes VoyageAI [ 4 ], a commercial embedding model

specialized at code embedding and handling long context, and GritLM [ 38 ], an open-source 7B parameter LLM trained

jointly to learn embedding and generation tasks. The context length of VoyageAI is 32,000; GritLM leverages a sliding

window and can handle arbitrary length inputs [ 38 ]. The dimensions of embedding are 1,024 and 4,096. We select

GritLM mainly due to its good performance and fast inference speed, as it leverages the KV caching technique [ 38 ] to

reduce the inference time cost.

**Achieving the Full Context Embedding using Hierarchical Embedding with GritLM** . Since the financial cost of

running VoyageAPI depends on the input tokens, it is not cost-effective to apply Voyage to retrieve the full repository.

To this end, we use GritLM as the backbone for building our method.

Although GritLM can handle arbitrary length inputs, the inference time cost of longer context are significantly

higher. Furthermore, our observation shows that keeping a longer context can result in a lower recall than when using a

shorter context. The reason may be that the training data for GritLM’s embedding model are usually shorter sentences,

the optimal context length we find is 512 tokens (Figure 5).

To minimize the truncation of the diff code, we propose the following hierarchical embedding approaches. First, we

split each diff code into files, then index the concatenation of the commit message and the first 512 tokens of each file

commit using the GritLM embedding: " *Commit message: [commit_msg], Diff code: [file_diff]* ". Second, during the query

time, we aggregate the file-level embeddings to compute multiple features on the similarity with the CVE. Furthermore,

we leverage the BM25 score between the CVE and each file diff to guide the aggregation. In particular, we propose the

following aggregated features (Table 1):

- 2. *𝑚𝑎𝑥* *𝑓* ∈ *𝑑𝑖𝑓𝑓* *𝑐𝑜𝑠* ( *𝑣* *𝑓* *, 𝑣* *𝐶𝑉𝐸* ) : the maximum cosine similarity between the CVE and all file embeddings in the diff

- 3. *𝑐𝑜𝑠* ( *𝑣* *𝑡𝑜𝑝* −1 ∈ *𝑑𝑖𝑓𝑓* *, 𝑣* *𝐶𝑉𝐸* ) : the cosine similarity between the CVE and the top-1 file embedding in the diff, where

the top-1 file is selected based on the BM25 score between the CVE and the files;

- 4. *𝑐𝑜𝑠* ( *𝑣* *𝑡𝑜𝑝* −2 ∈ *𝑑𝑖𝑓𝑓* *, 𝑣* *𝐶𝑉𝐸* ) : the cosine similarity between the CVE and the mean of the top-2 file embedding in

the diff, where the top-2 files are selected based on the BM25 score between the CVE and the files;

**Improving GritLM Embedding using Instructions** . To improve GritLM Embedding, we use a prompt for both

the CVE and file/commit embedding to resemble the prompt used in GritLM’s training. The prompt for each commit

message and file diff is: " *This is a commit (commit message + diff code) of a repository. Represent it to retrieve the patching*

*commit for a CVE description: [commit_msg_and_diff]* "; the prompt for the CVE is: " *Represent this CVE description to*

*retrieve the commit (commit message + diff code) that patches this CVE.* ". In our preliminary study using 78800 commits

and 96 CVEs of apache/tomcat, we find that this instruction can improve the recall@100 of GritLM embedding by 15%,

compared to without the instruction.

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 7

**Discussion of Hierarchical Embedding** . By splitting the diff code into files, our method thus cannot capture cross-file

contextual dependencies. That is, when the model needs to reason the relevance by examining two files simultaneously.

We argue that such dependencies are not crucial for patch retrieval, as patch retrieval primarily aims to localize the

code changes referenced in the CVE description, rather than reasoning across the entire diff’s context.

As Table 3 shows the average tokens in each file in our datasets are 2,454 and 2,577, thus even our hierarchical

embedding approach needs to truncate the files. However, unlike existing work [ 30 ], our approach ensures that every

file is considered; it also preserves the structure of each file. Our hierarchical embedding method can be extended to

incorporate the full context by adding more embeddings per file, but such extensions come at the cost of increased

indexing overhead.

One advantage of our hierarchical embedding approach is that once the file embeddings are indexed, the aggregated

features can be computed efficiently using matrix operation (matrix multiplication, segmented matrix reduction, matrix

grouping) between the CVE embeddings and the commit/file embeddings, making it easy to introduce and test new

features quickly.

**3.4** **Bridging CVE-Patch Gap using NER and In-Repo Search**

After building the code embedding, we identify that one challenge for matching the CVE description and diff code is

that some entities mentioned in the CVE description are not in the diff code. For example, for CVE-2021-41079 [ 40 ]:

" *Apache Tomcat 8.5.0 to 8.5.63, 9.0.0-M1* - · · *to use* ***NIO+OpenSSL*** *or* ***NIO2+OpenSSL*** - · · " (Table 2), NIO2 is not found in

the patching commit [ 41 ]. Instead, it refers to related files under the *apache/tomcat/java/util/net* directory. Since the

patching commit is under the same directory, searching for " *NIO2* " can help bridge the gap between the CVE description

and the patching commit.

Table 2. Prompt and example for NER and In-Repo search.

Prompt *Given the CVE description of [software name], extract*
*the entities (variable, file, method, class, other modules)*
*in the code to help retrieve the patching commit of the*
*vulnerability. Extract entities that are camel/snake case*
*or similar, e.g., connected using -, :, +. Only output words*
*in the description. Do not extract: (1): words describing*
*the software name/versions; (2): words describing the*
*vulnerabilit* *y* *and ex* *p* *loit method.*
Example *Apache Tomcat 8.5.0 to 8.5.63, 9.0.0-M1 to 9.0.43 and*
(CVE-2021- *10.0.0-M1 to 10.0.2 did not properly validate incom-*
41079) *ing TLS packets. When Tomcat was configured to use*

***NIO+OpenSSL*** *or* ***NIO2+OpenSSL*** *for TLS, a specially*
*crafted packet could be used to trigger an infinite loop*
*resultin* *g* *in a denial o* *f* *service.*

*java/org/apache/tomcat/* ***util*** */* ***net*** */Nio2Channel.java*

" *java/org/apache/tomcat/* ***util*** */net/Nio2Endpoint.java*
Search " *NIO2*

*java/org/apache/coyote/ajp/AjpNio2Protocol.java*
*j* *ava/or* *g* */a* *p* *ache/tomcat/* ***util*** */* ***net*** */SecureNio2Channel.* *j* *ava*
Patch file *j* *ava/or* *g* */a* *p* *ache/tomcat/* ***util*** */* ***net*** */o* *p* *enssl/O* *p* *enSSLEn* *g* *ine.java*

To bridge the gap, we propose to use Named Entity Recognition (NER) to extract the entities in the CVE description,

then use GitHub’s code search API [ 18 ] to find file paths containing the entities. The entities we look for are the

Manuscript submitted to ACM


-----

8 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

exact phrases that help uniquely identify the location of the patch. We avoid extracting words describing the software

name/versions, the vulnerability, the exploit method, and natural language phrases. We query OpenAI’s GPT-4o API

with the prompt in Table 2 and 8 input/output examples. We encode the file paths using the VoyageAI embedding [ 4 ],

and compute the cosine similarity between the NER-path and the file paths in each commit as the path similarity

feature (Table 1). We also compute the Jaccard index [ 57 ] between the NER-paths and the commit paths as another

path similarity feature. To address cases where paths may be incomplete or only partially specified—such as compar
ing src/utils/file.py with file.py, we additionally incorporate filename similarity computed through sequence

matching using the difflib package in Python.

**3.5** **Learning to Rank**

After building the features, we use learning-to-rank to learn how to combine the features. We use the LightGBM

library [ 26 ] for its faster training speed (e.g., comparing with similar libraries [ 8 ]). More specifically, we use the

LambdaRank algorithm [ 7 ]. We further perform automated hyperparameter tuning using the FLAML library [ 53 ].

FLAML (Fast and Lightweight AutoML) is employed to automatically optimize hyperparameters such as learning

rate, number of leaves, and minimum data per leaf of the LambdaRank model. The model was trained to optimize

evaluation metrics including NDCG, recall, and Mean Reciprocal Rank (MRR) at retrieval positions 10, 100, and 1000.

We run FLAML for approximately one hour, conducting around 30 trials to identify the optimal hyperparameter set.

The search space of FLAML is defined as follows: learning rate: tune.loguniform(0.005, 0.2), number of leaves:

tune.qrandint(20, 64, q=1), minimum data per leaf: tune.qrandint(5, 50, q=1) . The optimal hyperparameters

are then used to train the final model.

**Preparing the Training Data** . To reduce the indexing cost of the training data, for each CVE for training, we randomly

sample a set of 1000 commits as the negative commits, including 500 hard negative commits which are the top-500

commits ranked by BM25+Time, and 500 random negative commits. Notice we only sample the negative commits for

training, but not for testing. The negative sampling makes the pos:neg ratio in the test dataset much smaller than the

training set; however, we observe that the gap in the pos:neg ratio has minimal impact on the learning-to-rank result,

since LTR algorithms (e.g., LambdaRank, LambdaMART [ 7 ]) are designed to handle the imbalance between the positive

and negative samples.

**4** **Evaluation**

**4.1** **Research Questions**

Our evaluation for SITPatchTraceris guided by the following research questions:

- **RQ1** : How effective is SITPatchTracerin tracing the patching commit?

- **RQ2** : How costly is SITPatchTracerin tracing the patching commit?

- **RQ3** : How does each feature in SITPatchTracercontribute to the final performance?

**4.2** **Dataset Construction**

**Constructing a New Dataset** . To evaluate the effectiveness of SITPatchTracerand existing work, we use two datasets:

(1) PatchFinder, a dataset collected by the PatchFinder paper [ 30 ], and (2) GirtHubAD, a dataset we construct based on

the GitHub commit URL in the reference links of the GitHub Advisory database [ 17 ]. The reason we need to collect a

second dataset is that there are only 510 repositories in the PatchFinder dataset, and the distribution for the number

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 9

of CVEs are highly skewed, with 24 repositories (e.g., Linux, tensorflow) containing 80% CVEs. As a result, using

PatchFinder alone cannot show the method’s effectiveness on the majority OSS repositories with a smaller number of

CVEs.

**The GitHubAD Dataset** . To comprehensively evaluate SITPatchTracer’s effectiveness on OSS repositories, we collect

a new dataset named GitHubAD based on the GitHub Advisory database [ 17 ]. GitHub Advisory database maintains CVE

information by organizing vulnerabilities into ecosystems (e.g., Maven, PyPI, NPM). For each CVE, GitHub AD clones

the metadata information from NVD [ 2 ] (e.g., CVE description, patch, fixed version), and allows users to easily add or

update the metadata information by submitting pull requests, which are then reviewed and validated by security experts

in GitHub’s security team. For each CVE, we collect the commit URLs in the reference links as the patching commits

for this CVE, e.g., [ 19 ], which is similar to the data collection process of VFCFinder [ 14 ]. Arguably, the reference links

may include commits that have not been officially confirmed by the vendor as the patch. However, since these commits

are all relevant to the CVE, a higher ranking score indicates that the model is also effective in retrieving the correct

patching commit.

**Train/Test Split for Evaluating Challenging Test Repositories** . To evaluate SITPatchTracerand existing work,

we split both PatchFinder and GitHubAD into training and testing sets by repositories. That is, each repository only

belongs to one of the training and testing dataset, but not both. This approach follows previous work [ 9, 14 ] and it

reduces the contamination between the training and testing datasets.

On the other hand, while existing work often splits the train/test datasets randomly (e.g., PatchFinder [ 30 ] randomly

splits the dataset into 8:1:1 train/validate/test), we argue that this approach may over-estimate the model performance,

since the challenging repositories have a higher chance to locate in the training dataset. To this end, we propose to sort

the repositories based on the difficulty of the patch retrieval task, and use the more challenging repositories for test,

and the easier repositories for training.

To find out which repositories are more challenging, we leverage the number of commits in the repository and

the scores of BM25+Time (Section 3.2). The intuition is that for repositories containing a larger number of commits,

if BM25+Time cannot achieve a high recall, the repository is likely more challenging; on the other hand, it is easier

for smaller repositories with fewer commits to achieve a higher retrieval score, therefore the difficulty score should

penalize the repositories with smaller number of commits. In addition, we also consider the number of CVEs in the

repository to make the indexing more cost-effective for VoyageAI [4].

More specifically, we introduce the following difficulty metric (Equation 1).

*𝐷𝑖𝑓𝑓𝑖𝑐𝑢𝑙𝑡𝑦* = *𝑀𝑅𝑅* *𝐵𝑀* 25+ *𝑇𝑖𝑚𝑒* + 0 *.* 1 × *𝑅𝑒𝑐𝑎𝑙𝑙* @100 *𝐵𝑀* 25+ *𝑇𝑖𝑚𝑒*

+ 0 *.* 1 × *𝑅𝑒𝑐𝑎𝑙𝑙* @500 *𝐵𝑀* 25+ *𝑇𝑖𝑚𝑒*

+ 0 *.* 1 × *𝑅𝑒𝑐𝑎𝑙𝑙* @1000 *𝐵𝑀* 25+ *𝑇𝑖𝑚𝑒* (1)

For repositories with more than 5000 commits, we select the most difficult repositories based on *𝐷𝑖𝑓𝑓𝑖𝑐𝑢𝑙𝑡𝑦* ; for

repositories with less than 5000 commits, we select the difficult repositories based on *𝐷𝑖𝑓𝑓𝑖𝑐𝑢𝑙𝑡𝑦* / *𝑚𝑎𝑡ℎ.𝑙𝑜𝑔* ( 100 +

# *𝑐𝑜𝑚𝑚𝑖𝑡𝑠* ) . Within the difficult repositories, we select the ones with the largest number of CVEs. For PatchFinder, this

leads to 101 test repositories (629 CVEs), for GitHubAD, this leads to 148 test repositories (952 CVEs). The number of

repositories to keep in the test datasets are chosen mainly based on the cost for indexing the VoyageAI embedding

model [ 4 ]. For every dataset, we leave all the remaining repositories as the training dataset. The statistics of the datasets

Manuscript submitted to ACM


-----

10 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

Table 3. Statistics of the datasets used in this paper

Statistics PatchFinder GitHubAD

Tokens (commit msg) 56 44
Tokens per diff (white space) 3,498 3,217
Tokens per diff (voyage tokenizer) 12,370 16,236
Tokens per diff (GritLM tokenizer) 14,293 19,241
Tokens p er file ( GritLM tokenizer ) 2,454 2,577
# repos (train) 343 477 (sampled)
# repos (test) 101 148
# CVEs (train) 3,682 1,290
# CVEs (test) 629 952
# commits 1,597,753 1,846,915
# commits (train) 918,125 617,686
# commits ( test ) 679,628 1,229,229
Pos:ne g 1:20415 1:13449

are shown in Table 3. For GitHubAD’s training set, we randomly sample 477 repositories to reduce the indexing cost for

training.

**4.3** **Experiment Setup**

Our experiments are conducted on a university-managed high-performance computing (HPC) cluster where the jobs

are managed by SLURM. The cluster consists of 2 nodes, each containing 4 x NVIDIA L40S GPUs of 46G GPU Ram,

64 CPUs, and 250G CPU Ram; and 4 nodes, each containing 4 x NVIDIA H100 GPUs of 80G GPU Ram, 64 CPUs and

250G CPU Ram. We use 3TB storage space to store the repositories, extracted features, and indices of ElasticSearch,

SITPatchTracerand the baseline models. We run the experiments of GritLM on the L40S nodes, while the experiments

on PatchScout, PatchFinder, and VFCFinder are run on the L40S nodes and H100 nodes. We compare the runtime cost

of all methods by rerunning all experiments on the L40S nodes (Table 5).

**4.4** **Baselines**

We compare SITPatchTracerwith the following baselines:

- **PatchFinder Phase-1** [ 30 ]: PatchFinder leverages a two-phase approach for patch retrieval. We only compare

with the Phase-1 model, since the Phase-2’s ranking score is upper-bounded by the recall of Phase-1. We have

optimized PatchFinder’s code to make it more memory-efficient for our setting for larger repositories; [2] ;

- **PatchScout** [ 49 ]: We compare against PatchScout using PatchFinder’s implementation [ 51 ], as PatchScout’s

original implementation is unavailable;

- **VFCFinder** [ 14 ]: VFCFinder requires a prohibitively higher cost than any other method (Table 5), since VFCFinder

is designed to operate directly with the raw directory. While it is possible for us to refactor VFCFinder to

use our processed data, it would require a significant change of the code and it is prone to error. As a re
sult, we compare SITPatchTracerand VFCFinder on a subset of 8 repositories from GitHubAD (234 CVEs):

2 The original implementation has an out-of-memory error by attempting to store the entire final layer (a 3-dimensional tensor) of all commits of a
repository in the CPU memory.

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 11

mindsdb/mindsdb, apache/tomcat, spring-projects/spring-framework, vantage6/vantage6, answerde
v/answer, directus/directus, OpenNMS/opennms, and cloudfoundry/uaa . The 8 repositories are among the

most challenging repositories in GitHubAD.

- **Voyage-3** [ 4 ]: VoyageAI is a commercial company focusing on training embedding models. As of Nov 2024,

Voyage-3 was the top-1 model on the CoIR [ 31 ] benchmark. The cost of Voyage-3 is $0.06 per 1M tokens. In our

experiment, the estimated cost for indexing each 10,000 commits using Voyage-3 is $1.8. To save the cost of

running Voyage-3, we truncate each commit diff code as the first 50,000 characters (which is approximately 15,000

to 20,000 tokens by the Voyage tokenizer). After the truncation, the cost for running the entire PatchFinder test

dataset is approximately $122, and for the GitHubAD test dataset is $220. On the other hand, Voyage-code-3 and

Voyage-3-large further outperformed Voyage-3 on the CoIR benchmark since they use larger models. However,

their prices are 3 times higher than Voyage-3. We do not include them in our experiment since they are not

cost-effective for patch retrieval.

**Discussion on VCMatch** . Besides the baselines above, another candidate comparative method is VCMatch [ 54 ], which

extends the PatchScout framework by adding additional features and fine-tuning the BERT model. VCMatch’s score is

stronger than PatchScout and weaker than PatchFinder on the PatchFinder dataset (as reported by the PatchFinder

paper [ 30 ]). However, although VCMatch’s code is available [ 50 ], there remains a significant challenge in successfully

compiling and reproducing the code, even after reaching out to the authors of both VCMatch and PatchFinder. As a

result, we skip the comparison with VCMatch.

Table 4. Comparison between SITPatchTracerand baselines on two datasets: GitHubAD and PatchFinder [ 30 ]. * denote the method
does not require training. Grit_head2 is Feature 2 in Table 1. t and p are the t-value and p-value of the paired t-test between
SITPatchTracerand Voyage.




Manuscript submitted to ACM


-----

12 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

**4.5** **Evaluation Metrics**

For evaluating the effectiveness of SITPatchTracerand the baselines, we use the following metrics:

- **Mean Reciprocal Rank**, which evaluates the model’s performance of the top-1 retrieved commits;

- **Recall@k**, which evaluates the model’s completeness to include all patching commits in the top-k positions.

Recall@k is frequently used by previous work [ 14, 30 ], and it is a crucial metric for evaluating the model’s

potential as a *pre-ranking method* .

- **NDCG@k** . Compared to recall, NDCG focuses more on ranking the patching commits to the higher position,

thus it can be used to evaluate the model’s potential for the final ranking. For CVEs with more than one patching

commit, it provides finer-grained evaluation than MRR.

**5** **Experimental Results**

**5.1** **RQ1: Effectiveness Analysis**

Our overall evaluation results are shown in Table 4, where we compare SITPatchTracerwith PatchFinder, PatchScout,

and Voyage. SITPatchTracer’s result in Table 4 is highlighted in bold if it significantly outperforms Voyage. We

annotate 0-shot methods using * to indicate that they do not require any training. To speed up the calculation, we apply

the BM25+Time top-10000 pre-ranking to all methods. Since the recall of BM25+Time’s top-10000 commits are 96.2%

and 94.5% for PatchFinder and GitHubAD, this approach has minimal impact on the final evaluation.

From Table 4 we can make the following observations. First, PatchScout’s scores are lower than the other methods. It

is also lower than BM25+Time, showing that the traditional count-based feature extraction method is not sufficient

to capture the semantic relatedness between CVE and the patch. Second, while PatchFinder shows better scores than

PatchScout, it is lower than Voyage, Grit_head2, and SITPatchTracer. This result may be because the CodeReviewer

model is not optimized for text embedding tasks; in addition, since PatchFinder truncated each diff code to the first

512 tokens, it is prone to missing key information in the diff code. Finally, SITPatchTracersignificantly outperforms

Voyage on the GitHubAD dataset; on the PatchFinder dataset, SITPatchTraceroutperforms Voyage on MRR and

Recall@10; on the other metrics, SITPatchTracer’s improvement is not significant according to the results of the paired

t-test (Table 4). These results show that SITPatchTraceris effective in retrieving the patching commits by matching

or outperforming the state-of-the-art commercial embedding model. The lower improvement on PatchFinder may be

because GritLM contains less code-specific training data [ 20 ], whereas Voyage-3’s training data includes code [ 52 ].

One potential approach for improving SITPatchTraceris to fine-tune the GritLM to better understand the code and

vulnerability data. For example, by training the model to understand the association between semantic similarity

between the CVE description and the diff code (Section 7).

Due to the higher time cost of VFCFinder (Table 5), we only compare SITPatchTracerand VFCFinder on a subset of 8

repositories from GitHubAD (234 CVEs). The results are shown in Figure 3. We can see that SITPatchTraceroutperforms

VFCFinder on all metrics. In particular, VFCFinder’s recall stays the same across all top k. This is because VFCFinder

only retrieves a small set of commits within the fixed version, thus the recall will not keep increasing. As a result,

VFCFinder is not effective as a pre-ranking approach like the other methods. By inspecting the output of VFCFinder, we

find that many CVEs (60%) are not processed successfully due to the failure in its pipeline to handle the fixed version or

the fixed version is not available.

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 13

Fig. 3. Comparison between SITPatchTracerand VFCFinder [14] on 8 repositories in the GitHubAD (234 CVEs) (Section 4.4)

Table 5. Time/financial cost of the main components of SITPatchTracerand baselines. Each cell shows the cost of processing 10,000
commits of one CVE. We benchmark the runtime of SITPatchTracer, PatchScout, and PatchFinder on the same node equipped with
an NVIDIA L40S GPU for consistency

Method Ste p Time

Index BM25 20s

Index BM25 file 48s

SITPatchTracer Index GritLM 2 mins

Index GritLM file 29min
Querying GritLM 14s
Querying GritLM 72s
file

Path embeddin g $0.021

Feature Extraction 9min
PatchScout
Learnin g 2.6min
PatchFinder Phase-1 9 mins

Vo y a g e Commit embeddin g $1.8
VFCFinder Feature Extraction 50min

**RQ1** : SITPatchTracersignificantly outperforms PatchFinder [ 30 ], PatchScout [ 49 ], VFCFinder [ 14 ]. SITPatch
Traceroutperforms VoyageAI by 13% and 28% on Recall@10, and it is significantly more cost-effective than

VoyageAI.

**5.2** **RQ2: Cost Analysis**

We report the time and financial cost of the main components of SITPatchTracerand the baselines in Table 5. The

time cost is measured by the time taken to process 10,000 commits of one CVE. The financial cost is measured by the

cost of using the commercial API for processing 10,000 commits. We benchmark the runtime of SITPatchTracer,

Manuscript submitted to ACM


-----

14 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

Fig. 4. Performance changes (MRR, Recall@K, and NDCG@K) on the GitHubAD (top) and PatchFinder dataset (bottom), when
different feature groups are incrementally removed.

PatchScout, and PatchFinder on the same L40S node for consistency, despite their experiments being run on different

nodes (Section 4.3).

From Table 5, we can see that SITPatchTracertakes about 40 mins to process every 10,000 commits when using an

L40S GPU, including all the indexing and querying time. Although SITPatchTracer’s time cost is higher than the

baselines, considering that the indexing cost is a one-time effort, this time cost is manageable for practical deployment

to large repositories.

**RQ2** : SITPatchTracertakes about 40 mins to process every 10,000 commits when using an L40S GPU, which

is practical for large repositories. The financial cost of SITPatchTraceris $0.021 for processing 10,000 commits,

significantly lower than the $1.8 cost of VoyageAI [ 4 ], and it can be reduced by replacing the path embedding with

open-source models.

**5.3** **RQ3: Evaluating the Effectiveness of SITPatchTracerComponents**

*5.3.1* *Ablation Study on Features.* To study how each feature affects the effectiveness of SITPatchTracer, in this

section, we conduct an ablation study on different features in SITPatchTracer. While the feature importance can

be analyzed using explainable machine learning methods [ 36, 44 ], we find such analysis results are less stable and

more difficult to explain. We thus conduct the ablation study by removing one feature group at a time and observing

the performance changes. The feature groups are shown in Table 1. We start from the model trained with the best

hyperparameter (found by FLAML) using the full feature set. Each time, one feature group is removed and we retrain

the model using the same hyperparameters.

The results of our ablation study on the PatchFinder and GitHubAD are shown in Figure 4. We can see that every

feature group positively contributes to the overall performance of SITPatchTracer. In particular, for the recall, the

drop brought by removing the embedding features is the most significant, followed by the time features and the path

features. This result shows that embedding is crucial for improving the recall. For NDCG, this feature is more important

to GitHubAD than the PatchFinder dataset.

*5.3.2* *Evaluating the Effect of Context Lengths of GritLM.* When building the hierarchical embedding, we truncate

each file of the diff code to the first 512 tokens. How does the context length affect the performance of GritLM and

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 15

Fig. 5. Evaluating the effect of context lengths of GritLM on the apache/tomcat dataset. The context length varies from 512 to 4096.

SITPatchTracer? To answer this question, we study the retrieval scores using GritLM on apache/tomcat, a repository

with 78800 commits and 96 CVEs. We vary the context length from 512 to 4096, the results are shown in Figure 5.

From Figure 5, we can see that the context length has a negative impact on the performance of GritLM. The MRR

and Recall@10 scores drop significantly when the context length is larger than 512. We believe this counter-intuitive

result is due to the fact that the embedding model of GritLM is trained on pairs of text that are relatively short (e.g.,

the training data of GritLM [ 20 ]). As a result, longer context lengths may hurt the model’s performance instead. To

extend the context length of SITPatchTracer, we can either use a model that better handles long context, or index

more chunks in the same file, but the latter approach will increase the cost of indexing.

Table 6. Manual annotation results of NER accuracy

Precision Recall

AD train 1.0 0.81

AD test 1.0 0.82

PatchFinder train 1.0 0.85

PatchFinder test 1.0 0.76

*5.3.3* *Evaluating the Accuracy of NER.* We evaluate the accuracy of the NER model for identifying named entities in

the CVE description (Section 3.4). To evaluate the NER model, one author samples 100 CVEs from each of the training

and test sets of each dataset. The author manually inspects the CVE description to annotate the ground truth entities

based on the labeling result of the GPT model. The criteria are whether the entities follow the requirements in the GPT

prompt (Section 3.4), i.e., it refers to a file name, class name, function name, variable name, etc. unique to the commit,

and it does not include the software name/version, etc.

Manuscript submitted to ACM


-----

16 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

The results of NER’s accuracy are shown in Table 6, where the precision means the CVE does not contain any wrongly

annotated entities, and the recall means the GPT results include all the correctly annotated entities. The precision is

more important than the recall, since incorrect entities will introduce noise to the path feature. From Table 2 we can see

that the annotated entities have a high precision, and a reasonably high recall.

**RQ3** : SITPatchTracer’s performance is more significantly affected by the embedding than the time and path

features. The context length of GritLM has a negative impact on the performance of SITPatchTracer.

**6** **Related Work**

In this section, we summarize the related work.

**Security Vulnerability Data Management** . One critical defense against security exploits to OSS is to enable the

knowledge-sharing of known vulnerabilities. Security knowledge sharing of open-source vulnerabilities is led by the

efforts of security advisory databases including NVD [ 2 ], GitHub Advisories [ 17 ], Snyk [ 46 ], GitLab [ 3 ], and OSV [ 42 ].

Such knowledge includes the CVE description, the affected software and version, the patch location, the severity score,

etc.

Due to the tremendous growth of CVEs, existing work builds automated systems to maintain the metadata information.

The patching commit serves as an important meta-information source to help with these automated tasks. First, affected

software. Existing works automatically extract affected package information based on the CVE description. For example,

using named entity recognition (NER) [ 5, 13, 28, 63 ], extreme multi-class classification [ 11, 21, 34 ], large language model

ranking and generation [ 9, 10 ], and learning to rank [ 59 ]. When the affected package name is not exactly mentioned

in the CVE description, the patch information can help justify the predicted affected package. For example, Wu et

al. [ 59 ] recommend the affected package name by searching through the Maven central repository for the package

closest to the code change in the patch. Second, affected version. To reduce false alarms and narrow down the affected

version range, existing works study how to identify the vulnerability-inducing commit [ 6, 60 ] by starting from the

patching commit and tracing back along the version history graph using the SZZ algorithm [ 6, 60 ]. Third, severity

score. The severity score of a CVE is important information for managing the priority for fixing and mitigating different

vulnerabilities. Existing work leverages the diff code content to automatically assess the severity (i.e., CVSS) score of

the vulnerability [29].

In summary, the patching commit is an important information source for security vulnerability data management.

To improve the traceability of vulnerability patches, Xu et al. [ 62 ] propose to extract the patching commit URL by

crawling the reference links of a CVE; Sun et al. [48] localize the patch at the file level.

**Text Embedding Models** . Text embedding models are a special type of models that learn how to represent the entire

text into one vector embedding. Different from generative LLMs, text embedding models are trained by encoding two

pieces of text using the same model, and then learning their similarity. Text embedding allows the cosine similarity

calculation to be decoupled between the query and the candidate. Therefore, we can first index all the candidates in a

repository as candidate embeddings, then rank all candidates efficiently using matrix multiplication or approximate

nearest neighbor search [ 25, 27, 35, 61 ]. State-of-the-art text embedding models such as Voyage AI [ 4 ] achieve 90%

NDCG@10 [37, 39] on the CoIR [31] benchmark (a benchmark for evaluating code retrieval models).

An important requirement of text embedding models is to improve the inference speed for fast retrieval. Many

embedding models are built upon the smaller language models such as all-mpnet [ 47 ]. To improve the effectiveness

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 17

of retrieval, recent works develop embedding models based on large language models. For example, GritLM [ 38 ] is

based on the Mistral-7B models. GritLM is trained with a joint objective function for the model to learn to generate and

embed at the same time, improving the generality of the embedding. It leverages the KV caching technique to reduce

the inference time cost. GritLM is trained with general-domain datasets as well as several code-related datasets such as

splash question to sql and splash explanation to sql [ 20 ]. On the other hand, existing works develop embedding models

that focus on the code data, e.g., Voyage-Code-3 [ 4 ] and CodeXEmbed [ 33 ]. We choose to use GritLM for building

SITPatchTracerdue to its good performance, high inference speed and low financial cost.

**7** **Discussions and Future Work**

**Improving the Semantic Matching between CVE and Code** . Although SITPatchTracerachieves a higher recall on

the GitHubAD and PatchFinder datasets, there remain challenges in improving the semantic matching between the

CVE description and the commit diff code. For example, how to bridge the gap between the vulnerability type (e.g.,

" *this could lead to an infinite loop and denial-of-service* ") with the diff code containing the infinite loop? This question is

challenging for GritLM since it contains limited training data on security vulnerabilities. For example, we can fine-tune

GritLM’s embedding model using pairs of (CVE, vulnerable function) from existing vulnerability datasets [ 16, 55, 66 ].

This approach enables the model to learn the associations between vulnerable code and the vulnerability types.

**8** **Threats to Validity**

**External Threats** . External threats include the reproducibility of baselines such as PatchFinder [ 30 ], PatchScout [ 49 ],

and VFCFinder [ 14 ]. We have made efforts to reproduce these baselines using the code provided by the original authors.

However, we encountered challenges, particularly when handling large datasets. To ensure successful execution, we

have refactored parts of the baseline implementations to align with our hardware configurations (e.g., mitigating

out-of-memory errors). These modifications, while necessary for execution, may introduce deviations from the original

implementations, potentially affecting direct comparability.

**Internal Threats** . Internal threats include the potential bias brought by our datasets. For example, for GitHubAD

we use the reference commit links as the patching commits, which may not be officially confirmed. However, since

these commits are all relevant to the CVE, we argue that such bias has minimal threats to the fair comparison between

models.

**9** **Conclusion**

In this work, we introduce SITPatchTracer, a scalable and effective patch retrieval system for security vulnerabilities

that can scale to large-scale OSS repositories. SITPatchTracerleverages the GritLM embedding model as the backbone,

it leverages a hierarchical embedding technique to reduce the truncation of the commit diff code. It further leverages

pre-ranking using ElasticSearch and the time difference between CVE and commit, as well as NER + path search to

bridge the gap by out-of-commit mentions in the CVE description. By combining these features in a learning-to-rank

framework, SITPatchTraceroutperforms existing work [ 14, 30, 49 ]; it further outperforms the Recall@10 of VoyageAI,

a commercial embedding API with state-of-the-art performance, by 13% and 28% on our two datasets. It is more

cost-effective than VoyageAI, and the time cost is manageable for practical deployment to large repositories.

Manuscript submitted to ACM


-----

18 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

**A** **An Empirical Study on the Accuracy of the Version Information**

In this section, we conduct an empirical study on the accuracy of the version information in the CVE description. We

sample 2,614 CVEs from NVD which contain at least one patch. We then calculate that starting from the extracted

version, the size of the version window to include the ground truth patch. The results are summarized in Table 7. We

find that the exact extracted version only includes 60.24% ground truth patches. As a result, filtering candidate commits

using the version information [14] is prone to a lower recall for patch retrieval.

Table 7. Statistics of the version distance between the mentioned fix version and the ground truth fix version

Ran g e Count Recall ( % )

0 1,677 60.24

±1 1,875 67.35

±5 2,136 76.72

±50 2,614 93.89

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 19

**References**

[[1] 2023. Common Vulnerabilities and Exposures Website. https://cve.mitre.org/](https://cve.mitre.org/)

[[2] 2023. NATIONAL VULNERABILITY DATABASE. https://nvd.nist.gov/](https://nvd.nist.gov/)

[[3] 2024. GitLab. http://gitlab.com](http://gitlab.com)

[4] [Voyage AI. 2024. voyage-3, voyage-3-lite: A new generation of small yet mighty general-purpose embedding models. https://blog.voyageai.com/](https://blog.voyageai.com/2024/09/18/voyage-3/)

[2024/09/18/voyage-3/](https://blog.voyageai.com/2024/09/18/voyage-3/)

[5] Afsah Anwar, Ahmed Abusnaina, Songqing Chen, Frank Li, and David Mohaisen. 2021. Cleaning the NVD: Comprehensive quality assessment,

improvements, and analyses. *IEEE Transactions on Dependable and Secure Computing* [(2021). https://ieeexplore.ieee.org/abstract/document/9601266](https://ieeexplore.ieee.org/abstract/document/9601266)

[6] Lingfeng Bao, Xin Xia, Ahmed E Hassan, and Xiaohu Yang. 2022. V-SZZ: automatic identification of version ranges affected by CVE vulnerabilities.

In *Proceedings of the 44th International Conference on Software Engineering* . 2352–2364.

[7] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. *Learning* 11, 23-581 (2010), 81.

[8] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In *Proceedings of the 22nd acm sigkdd international conference on*

*knowledge discovery and data mining* . 785–794.

[9] Tianyu Chen, Lin Li, Bingjie Shan, Guangtai Liang, Ding Li, Qianxiang Wang, and Tao Xie. 2023. Identifying vulnerable third-party libraries from

textual descriptions of vulnerabilities and libraries. *arXiv preprint arXiv:2307.08206* (2023).

[10] Tianyu Chen, Lin Li, Liuchuan Zhu, Zongyang Li, Guangtai Liang, Ding Li, Qianxiang Wang, and Tao Xie. 2023. Vullibgen: Identifying vulnerable

third-party libraries via generative pre-trained model. *arXiv preprint arXiv:2308.04662* (2023).

[11] Yang Chen, Andrew E Santosa, Asankhaya Sharma, and David Lo. 2020. Automated identification of libraries from vulnerability data. In *ACM/IEEE*

*International Conference on Software Engineering: Software Engineering in Practice* [. https://dl.acm.org/doi/abs/10.1145/3377813.3381360](https://dl.acm.org/doi/abs/10.1145/3377813.3381360)

[12] [Eric Chin. 2024. National Vulnerability Database: Opaque changes and unanswered questions. https://anchore.com/blog/national-vulnerability-](https://anchore.com/blog/national-vulnerability-database-opaque-changes-and-unanswered-questions/)

[database-opaque-changes-and-unanswered-questions/](https://anchore.com/blog/national-vulnerability-database-opaque-changes-and-unanswered-questions/)

[13] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context

learning. *arXiv preprint arXiv:2301.00234* [(2022). https://arxiv.org/abs/2301.00234](https://arxiv.org/abs/2301.00234)

[14] Trevor Dunlap, Elizabeth Lin, William Enck, and Bradley Reaves. 2024. VFCFinder: Pairing Security Advisories and Patches. In *Proceedings of the*

*19th ACM Asia Conference on Computer and Communications Security* . 1128–1142.

[[15] ElasticSearch. 2024. ElasticSearch. https://www.elastic.co/elasticsearch](https://www.elastic.co/elasticsearch)

[16] Jiahao Fan, Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. A C/C++ Code Vulnerability Dataset with Code Changes and CVE Summaries. In

*Proceedings of the 17th International Conference on Mining Software Repositories* (Seoul, Republic of Korea) *(MSR ’20)* . Association for Computing

[Machinery, New York, NY, USA, 508–512. doi:10.1145/3379597.3387501](https://doi.org/10.1145/3379597.3387501)

[[17] GitHub. 2024. GitHub Advisory Database. https://github.com/advisories](https://github.com/advisories)

[[18] GitHub. 2024. GitHub Code Search API. https://docs.github.com/en/rest?apiVersion=2022-11-28](https://docs.github.com/en/rest?apiVersion=2022-11-28)

[[19] GitHubAD. 2021. CVE-2021-41079’s GitHub page. https://github.com/advisories/GHSA-59g9-7gfx-c72p](https://github.com/advisories/GHSA-59g9-7gfx-c72p)

[[20] GritLM. 2024. MEDI2: GritLM’s training data. https://huggingface.co/datasets/GritLM/MEDI2/tree/main](https://huggingface.co/datasets/GritLM/MEDI2/tree/main)

[21] Stefanus A Haryono, Hong Jin Kang, Abhishek Sharma, Asankhaya Sharma, Andrew Santosa, Ang Ming Yi, and David Lo. 2022. Automated

Identification of Libraries from Vulnerability Data: Can We Do Better?. In *IEEE/ACM International Conference on Program Comprehension* [. https:](https://dl.acm.org/doi/abs/10.1145/3524610.3527893)

[//dl.acm.org/doi/abs/10.1145/3524610.3527893](https://dl.acm.org/doi/abs/10.1145/3524610.3527893)

[22] [Simon Hendery. 2024. Update delays to NIST vulnerability database alarms researchers. https://www.scmagazine.com/news/update-delays-to-nist-](https://www.scmagazine.com/news/update-delays-to-nist-vulnerability-database-alarms-researchers)

[vulnerability-database-alarms-researchers](https://www.scmagazine.com/news/update-delays-to-nist-vulnerability-database-alarms-researchers)

[[23] Chris Hughes. 2024. Death Knell of the NVD? https://www.resilientcyber.io/p/death-knell-of-the-nvd](https://www.resilientcyber.io/p/death-knell-of-the-nvd)

[24] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of

semantic code search. *arXiv preprint arXiv:1909.09436* (2019).

[25] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage

retrieval for open-domain question answering. *arXiv preprint arXiv:2004.04906* (2020).

[26] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient

boosting decision tree. *Advances in neural information processing systems* 30 (2017).

[27] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In *Proceedings of*

*the 43rd International ACM SIGIR conference on research and development in Information Retrieval* . 39–48.

[28] Philipp Kuehn, Markus Bayer, Marc Wendelborn, and Christian Reuter. 2021. OVANA: An approach to analyze and improve the information quality

of vulnerability databases. In *International Conference on Availability, Reliability and Security* [. https://dl.acm.org/doi/abs/10.1145/3465481.3465744](https://dl.acm.org/doi/abs/10.1145/3465481.3465744)

[29] Triet Huynh Minh Le, David Hin, Roland Croft, and M Ali Babar. 2021. Deepcva: Automated commit-level vulnerability assessment with deep

multi-task learning. In *2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)* . IEEE, 717–729.

[30] Kaixuan Li, Jian Zhang, Sen Chen, Han Liu, Yang Liu, and Yixiang Chen. 2024. PatchFinder: A two-phase approach to security patch tracing for

disclosed vulnerabilities in open-source software. In *Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis* .

590–602.

Manuscript submitted to ACM


-----

20 Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu

[31] Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. 2024. Coir: A comprehensive

benchmark for code information retrieval models. *arXiv preprint arXiv:2407.02883* (2024).

[32] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage

contrastive learning. *arXiv preprint arXiv:2308.03281* (2023).

[33] Ye Liu, Rui Meng, Shafiq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024. CodeXEmbed: A Generalist Embedding Model

Family for Multiligual and Multi-task Code Retrieval. *arXiv preprint arXiv:2411.12644* (2024).

[34] Yunbo Lyu, Thanh Le-Cong, Hong Jin Kang, Ratnadira Widyasari, Zhipeng Zhao, Xuan-Bach D Le, Ming Li, and David Lo. 2023. Chronos:

Time-aware zero-shot identification of libraries from vulnerability reports. In *International Conference on Software Engineering* [. https://doi.org/10.](https://doi.org/10.1109/ICSE48619.2023.00094)

[1109/ICSE48619.2023.00094](https://doi.org/10.1109/ICSE48619.2023.00094)

[35] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world

graphs. *IEEE transactions on pattern analysis and machine intelligence* 42, 4 (2018), 824–836.

[36] Edoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel Gallagher, and Georg Groh. 2022. SHAP-based explanation methods: a review for NLP

interpretability. In *Proceedings of the 29th international conference on computational linguistics* . 4593–4603.

[[37] MTEB. 2024. MTEB Leaderboard on Hugging Face. https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

[38] Niklas Muennighoff, SU Hongjin, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative representational

instruction tuning. In *ICLR 2024 Workshop: How Far Are We From AGI* .

[39] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. MTEB: Massive text embedding benchmark. *arXiv preprint arXiv:2210.07316*

(2022).

[[40] NVD. 2021. CVE-2021-41079. https://nvd.nist.gov/vuln/detail/CVE-2021-41079](https://nvd.nist.gov/vuln/detail/CVE-2021-41079)

[[41] NVD. 2021. CVE-2021-41079’s patching commit. https://github.com/apache/tomcat/commit/34115fb3c83f6cd97772232316a492a4cc5729e0](https://github.com/apache/tomcat/commit/34115fb3c83f6cd97772232316a492a4cc5729e0)

[[42] OSV. 2024. OSV Database. https://osv.dev/](https://osv.dev/)

[43] N Reimers. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. *arXiv preprint arXiv:1908.10084* (2019).

[44] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should i trust you?" Explaining the predictions of any classifier. In *Proceedings*

*of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining* . 1135–1144.

[45] Kedi Shen, Yun Zhang, Lingfeng Bao, Zhiyuan Wan, Zhuorong Li, and Minghui Wu. 2023. Patchmatch: A tool for locating patches of open source

project vulnerabilities. In *2023 IEEE/ACM 45th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)* . IEEE,

175–179.

[[46] Snyk. 2024. SNYK Open Source Vulnerability Database. https://security.snyk.io/](https://security.snyk.io/)

[47] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. Mpnet: Masked and permuted pre-training for language understanding. *Advances*

*in neural information processing systems* 33 (2020), 16857–16867.

[48] Jiamou Sun, Jieshan Chen, Zhenchang Xing, Qinghua Lu, Xiwei Xu, and Liming Zhu. 2024. Where is it? tracing the vulnerability-relevant files from

vulnerability reports. In *Proceedings of the IEEE/ACM 46th International Conference on Software Engineering* . 1–13.

[49] Xin Tan, Yuan Zhang, Chenyuan Mi, Jiajun Cao, Kun Sun, Yifan Lin, and Min Yang. 2021. Locating the security patches for disclosed oss vulnerabilities

with vulnerability-commit correlation ranking. In *Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security* .

3282–3299.

[[50] VCMatch. 2024. Reproduing Package for the VCMatch Model. https://figshare.com/s/0f3ed11f9348e2f3a9f8?file=32403518](https://figshare.com/s/0f3ed11f9348e2f3a9f8?file=32403518)

[[51] VFCFinder. 2024. VFCFinder replication package. https://github.com/s3c2/vfcfinder](https://github.com/s3c2/vfcfinder)

[52] [Voyage. 2024. voyage-3 & voyage-3-lite: A new generation of small yet mighty general-purpose embedding models. https://blog.voyageai.com/](https://blog.voyageai.com/2024/09/18/voyage-3/)

[2024/09/18/voyage-3/](https://blog.voyageai.com/2024/09/18/voyage-3/)

[53] Chi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. 2021. Flaml: A fast and lightweight automl library. *Proceedings of Machine Learning and*

*Systems* 3 (2021), 434–447.

[54] Shichao Wang, Yun Zhang, Liagfeng Bao, Xin Xia, and Minghui Wu. 2022. Vcmatch: a ranking-based approach for automatic security patches

localization for OSS vulnerabilities. In *2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)* . IEEE, 589–600.

[55] Xinda Wang, Shu Wang, Pengbin Feng, Kun Sun, and Sushil Jajodia. 2021. Patchdb: A large-scale security patch dataset. In *2021 51st Annual IEEE/IFIP*

*International Conference on Dependable Systems and Networks (DSN)* . IEEE, 149–160.

[[56] Wikipedia. 2024. 2017 Equifax Data Breach. https://en.wikipedia.org/wiki/2017_Equifax_data_breach](https://en.wikipedia.org/wiki/2017_Equifax_data_breach)

[[57] Wikipedia. 2024. Jaccard Index. https://en.wikipedia.org/wiki/Jaccard_index](https://en.wikipedia.org/wiki/Jaccard_index)

[[58] Wikipedia. 2024. Wikipedia page of Inverted index. https://en.wikipedia.org/wiki/Inverted_index](https://en.wikipedia.org/wiki/Inverted_index)

[59] Susheng Wu, Wenyan Song, Kaifeng Huang, Bihuan Chen, and Xin Peng. 2024. Identifying Affected Libraries and Their Ecosystems for Open

Source Software Vulnerabilities. In *Proceedings of the IEEE/ACM 46th International Conference on Software Engineering* . 1–12.

[60] Susheng Wu, Ruisi Wang, Kaifeng Huang, Yiheng Cao, Wenyan Song, Zhuotong Zhou, Yiheng Huang, Bihuan Chen, and Xin Peng. 2024. Vision:

Identifying affected library versions for open source software vulnerabilities. In *Proceedings of the 39th IEEE/ACM International Conference on*

*Automated Software Engineering* . 1447–1459.

[61] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest

neighbor negative contrastive learning for dense text retrieval. *arXiv preprint arXiv:2007.00808* (2020).

Manuscript submitted to ACM


-----

Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes 21

[62] Congying Xu, Bihuan Chen, Chenhao Lu, Kaifeng Huang, Xin Peng, and Yang Liu. 2022. Tracking patches for open source software vulnerabilities.

In *Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering* . 860–871.

[63] Guanqun Yang, Shay Dineen, Zhipeng Lin, and Xueqing Liu. 2021. Few-sample named entity recognition for security vulnerability reports by

fine-tuning pre-trained language models. In *Deployable Machine Learning for Security Defense: Second International Workshop* [. https://link.springer.](https://link.springer.com/chapter/10.1007/978-3-030-87839-9_3)

[com/chapter/10.1007/978-3-030-87839-9_3](https://link.springer.com/chapter/10.1007/978-3-030-87839-9_3)

[64] Junwei Zhang, Xing Hu, Lingfeng Bao, Xin Xia, and Shanping Li. 2024. Dual Prompt-Based Few-Shot Learning for Automated Vulnerability Patch

Localization. In *2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)* . IEEE, 940–951.

[65] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. *arXiv preprint*

*arXiv:1904.09675* (2019).

[66] Jiayuan Zhou, Michael Pacheco, Zhiyuan Wan, Xin Xia, David Lo, Yuan Wang, and Ahmed E Hassan. 2021. Finding a needle in a haystack: Automated

mining of silent vulnerability fixes. In *2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)* . IEEE, 705–716.

Manuscript submitted to ACM


-----
